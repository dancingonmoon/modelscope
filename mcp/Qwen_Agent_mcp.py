import pathlib
import ast
from qwen_agent.agents import Assistant
from qwen_agent.gui import WebUI
from qwen_agent.utils.output_beautify import typewriter_print
# `typewriter_print` prints streaming messages in a non-overlapping manner.

import qwen_agent
class Qwen_Agent_mcp:
    """
    åˆå§‹åŒ–Qwen-Agent,å¯é€‰å·¥å…·,ä¾‹å¦‚æœç´¢,code_interpreter, mcp, æµå¼è¾“å‡º;
    :return:
    """
    def __init__(self, model:str, model_server:str='dashscope', api_key:str=None,enable_thinking:bool=False,
                 enable_search:bool=True, force_search:bool=False, enable_source:bool=True,
                 enable_citation:bool=True, citation_format:bool="[ref_<number>]", search_strategy="pro",
                 code_interpreter:bool=False, mcp:dict|bool=None,
                 system_message:str='', description:str='', files:list[str]=None):
        """

        :param model: è­¬å¦‚: 'model': 'qwen-turbo-latest',   # è¾“å…¥0.0003å…ƒ;æ€è€ƒæ¨¡å¼0.006å…ƒ;éæ€è€ƒæ¨¡å¼0.0006å…ƒ
                            'model': 'qwq-plus-latest',   # è¾“å…¥0.0016å…ƒ;   è¾“å‡º0.004å…ƒ
                            'model': 'qwen-max-latest',   # è¾“å…¥0.0024å…ƒ;   è¾“å‡º0.0096å…ƒ
                            'model': 'qwen-plus-latest',  # è¾“å…¥0.0008å…ƒ;æ€è€ƒæ¨¡å¼0.016å…ƒ;éæ€è€ƒæ¨¡å¼0.002å…ƒ
        :param model_server: 'dashscope',æˆ–è€…url_base, è­¬å¦‚:'http://localhost:8000/v1'
        :param api_key: å¦‚ç¯å¢ƒå˜é‡ä¸­è®¾å®š,åˆ™æ­¤å¤„ä¸ºNone
        :param enable_thinking:
        :param enable_search: # å¼€å¯è”ç½‘æœç´¢çš„å‚æ•°
        :param force_search: # å¼ºåˆ¶å¼€å¯è”ç½‘æœç´¢
        :param enable_source: # ä½¿è¿”å›ç»“æœåŒ…å«æœç´¢æ¥æºçš„ä¿¡æ¯ï¼ŒOpenAI å…¼å®¹æ–¹å¼æš‚ä¸æ”¯æŒè¿”å›
        :param enable_citation: # å¼€å¯è§’æ ‡æ ‡æ³¨åŠŸèƒ½
        :param citation_format: # è§’æ ‡å½¢å¼ä¸º[ref_i]
        :param search_strategy: "pro"æ—¶,æ¨¡å‹å°†æœç´¢10æ¡äº’è”ç½‘ä¿¡æ¯
        :param code_interpreter:
        :param mcp: {}|Bool: Noneæ—¶ï¼Œä¸ºç¼ºçœåŠ è½½mcp:æ–‡ä»¶ç³»ç»Ÿï¼Œtime; Falseæ—¶ï¼Œä¸åŠ è½½mcp
        :param system_message: "æŒ‰ç…§ç”¨æˆ·éœ€æ±‚ï¼Œä½ å…ˆç”»å›¾ï¼Œå†è¿è¡Œä»£ç ...."
        :param description: 'ä½¿ç”¨RAGæ£€ç´¢å¹¶å›ç­”ï¼Œæ”¯æŒæ–‡ä»¶ç±»å‹ï¼šPDF/Word/PPT/TXT/HTMLã€‚'
        :param files: list,è­¬å¦‚: [os.path.join('.', 'doc.pdf')]
        :parm
        """
        tools  = []
        if mcp is None and mcp is not False:
            mcp = {
                "mcpServers": {
                    "filesystem": {
                        "command": "npx",
                        "args": [
                            "-y",
                            "@modelcontextprotocol/server-filesystem",
                            '.',
                            './mcp_docs/'
                        ]
                    },
                    "time": {
                        "command": "uvx",
                        "args": ["mcp-server-time", "--local-timezone=Asia/Shanghai"]
                    }
                }
            }
            tools.append(mcp)

        if code_interpreter:
            tools.append('code_interpreter')

        if model_server == 'dashscope':
            llm_config = {
            'model': model,
            'model_server': model_server,
            # 'api_key':  #  å¦‚æœç¯å¢ƒå˜é‡ä¸­å·²ç»è®¾å®š,åˆ™è¯¥é¡¹å¯ä»¥ä¸å¡«''  # **fill your api key here**
            'generate_cfg': {
                # When using the Dash Scope API, pass the parameter of whether to enable thinking mode in this way
                'enable_thinking': enable_thinking,
                'enable_search': enable_search, # å¼€å¯è”ç½‘æœç´¢çš„å‚æ•°
                'search_options': {
                    "forced_search": force_search, # å¼ºåˆ¶å¼€å¯è”ç½‘æœç´¢
                    "enable_source": enable_source, # ä½¿è¿”å›ç»“æœåŒ…å«æœç´¢æ¥æºçš„ä¿¡æ¯ï¼ŒOpenAI å…¼å®¹æ–¹å¼æš‚ä¸æ”¯æŒè¿”å›
                    "enable_citation": enable_citation, # å¼€å¯è§’æ ‡æ ‡æ³¨åŠŸèƒ½
                    "citation_format": citation_format, # è§’æ ‡å½¢å¼ä¸º[ref_i]
                    "search_strategy": search_strategy # "pro"æ—¶,æ¨¡å‹å°†æœç´¢10æ¡äº’è”ç½‘ä¿¡æ¯
                },

            },
        }
        else:
            llm_config = {
                # Use a model service compatible with the OpenAI API, such as vLLM or Ollama:
                'model': model,
                'model_server': model_server,  # base_url, also known as api_base
                'api_key': api_key
            }
        self.agent = Assistant(
            llm=llm_config,
            function_list=tools,
            name='my Assistant',
            system_message= system_message,
            description= description,
            files = files
            )





    def chat_once(self, query: str, query_file_path:str=None, messages_history:list=None):
        """
        å•æ¬¡å¯¹è¯ã€‚æ­¤å¤„è¾“å…¥query_file_pathï¼Œæ¥å—urlæˆ–è€…æ–‡ä»¶è·¯å¾„ï¼›é‡‡ç”¨çš„æ˜¯openAIå…¼å®¹çš„Fileæ ¼å¼, ä¾‹å¦‚ï¼š{messages = [{'role': 'user', 'content': [{'text': 'ä»‹ç»å›¾ä¸€'},
        {'file': 'https://arxiv.org/pdf/1706.03762.pdf'}]}]}ï¼› äº‹å®ä¸Šï¼Œç”±äºQwenæ¨¡å‹APIæœ¬èº«å¹¶ä¸æ”¯æŒFileæ ¼å¼ï¼Œè¿™æ˜¯ç”±Qwen-Agent SDKåšäº†è½¬æ¢åå®ç°ã€‚
        :param query:
        :param query_file_path: str;æ–‡ä»¶è·¯å¾„(æ”¯æŒdocx,pdf,ä¸æ”¯æŒjpg)ï¼Œæˆ–è€…url
        :param messages_history: list,
        :return:
        """
        # messages = [{'role': 'user', 'content': [{'text': 'ä»‹ç»å›¾ä¸€'},
        #             {'file': 'https://arxiv.org/pdf/1706.03762.pdf'}]}]

        if messages_history is None:
            messages_history = []
        # ä¾‹å¦‚ï¼Œè¾“å…¥è¯·æ±‚ "ç»˜åˆ¶ä¸€åªç‹—å¹¶å°†å…¶æ—‹è½¬ 90 åº¦"ã€‚
        # å°†ç”¨æˆ·è¯·æ±‚æ·»åŠ åˆ°èŠå¤©å†å²ã€‚
        if query_file_path is None:
            messages_history.append({'role': 'user', 'content': query})
        else:
            # æ–‡æœ¬æ–‡æ¡£çš„åç¼€åˆ—è¡¨,openAIæ ¼å¼ä¹Ÿæ”¯æŒdocx,doc,pdfï¼š
            text_extensions = ['.txt', '.md', '.csv', '.json', '.py', '.html', '.htm', '.xml', '.yaml', '.yml','.docx','.doc','.pdf']
            text_file = pathlib.Path(query_file_path)
            if text_file.exists() and text_file.suffix.lower() in text_extensions: # æ–‡æœ¬æ–‡æ¡£
                messages_history.append({'role': 'user', 'content': [{'text': query},
                                                                         {'file': query_file_path}]})
            elif query_file_path.startswith('http:') or query_file_path.startswith('https:'): # url
                messages_history.append({'role': 'user', 'content': [{'text': query},
                                                                     {'file': query_file_path}]})
            else:
                print("ä¸åˆæ³•çš„æ–‡ä»¶è·¯å¾„ï¼Œæˆ–è€…ä¸æ”¯æŒçš„éå›¾åƒæ–‡æ¡£åç¼€!")
                return messages_history

        response = []
        response_plain_text = ''
        print('Agent å›åº”:')
        for response in self.agent.run(messages=messages_history):
            # æµå¼è¾“å‡ºã€‚
            response_plain_text = typewriter_print(response, response_plain_text)
        messages_history.append(response)
        return messages_history

    def chat_continuous(self, ):
        """
        è¿ç»­å¯¹è¯ã€‚input()è¾“å…¥çš„é—®é¢˜ï¼Œå¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²;æˆ–è€…åˆ—è¡¨ï¼Œåˆ—è¡¨ç¬¬ä¸€ä¸ªå…ƒç´ ä¸ºé—®é¢˜ï¼Œç¬¬äºŒä¸ªå…ƒç´ ä¸ºæ–‡ä»¶è·¯å¾„æˆ–è€…url;æˆ–è€…å­—å…¸, ä¾‹å¦‚ï¼š {'text': 'ä»‹ç»å›¾ä¸€'},
                    {'file': 'https://arxiv.org/pdf/1706.03762.pdf'}
        :return:
        """
        message_history = []
        while True:
            query = ''
            file_path = None
            message = input('\nğŸ’¬ è¯·è¾“å…¥ä½ çš„æ¶ˆæ¯(è¾“å…¥exitæˆ–quité€€å‡º):')
            if message.lower() in ['exit','quit']:
                print("âœ… å¯¹è¯å·²ç»“æŸ")
                break
            message = ast.literal_eval(message) #  å®‰å…¨è§£æå­—ç¬¦ä¸²ä¸º Python å­—é¢é‡
            if isinstance(message, str):
                query = message
            if isinstance(message,list):
                query = message[0]
                file_path = message[1]
            if isinstance(message,dict):
                query  = message['text']
                file_path = message['file']

            message_dict = self.chat_once(query, query_file_path=file_path)
            message_history.append(message_dict)

        return message_history



    def webUI(self,user_name:str=None,user_avatar:str=None,
              input_placeholder:str=None,prompt_suggestions:list=None,
              share:bool=False,server_port:int=None,enable_emotion:bool=True):
        """
        gradio UI
        :param user_name:
        :param user_avatar: å›¾åƒè·¯å¾„
        :param input_placeholder:
        :param prompt_suggestions:
        :return:
        """
        if prompt_suggestions is None:
            prompt_suggestions = []
        if input_placeholder is None:
            input_placeholder = ''
        chatbot_config = {
            'input.placeholder': input_placeholder,
            'prompt.suggestions': prompt_suggestions
        }
        if user_name is not None:
            chatbot_config['user.name'] = user_name
        if user_avatar is not None:
            chatbot_config['user.avatar'] = user_avatar
        WebUI(self.agent,chatbot_config=chatbot_config).run(
            share = share,
            server_port = server_port,
            enable_mention = enable_emotion)



if __name__ == '__main__':

    qwen_agent  = Qwen_Agent_mcp(model='qwen-turbo-latest', mcp=False)
    # message_history = qwen_agent.chat_once("è¯·æ€»ç»“ä»Šå¤©çš„æ–°é—»10æ¡")
    message_history = qwen_agent.chat_continuous()
    # print(message_history)
    # qwen_agent.webUI(user_name=None,)






