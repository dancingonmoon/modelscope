{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2caeefe6-e81a-4ef8-9b82-98ac10208ef8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-17T10:37:17.636394Z",
     "iopub.status.busy": "2023-12-17T10:37:17.635927Z",
     "iopub.status.idle": "2023-12-17T10:37:18.533169Z",
     "shell.execute_reply": "2023-12-17T10:37:18.532412Z",
     "shell.execute_reply.started": "2023-12-17T10:37:17.636366Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer, Pipeline\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfeaf85-4afb-4c94-8622-299e18c2a8f0",
   "metadata": {},
   "source": [
    "## **tensorflow 训练方法**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "805c2524-dc9d-4692-8bcb-06293641e63e",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2023-12-14T06:31:09.897201Z",
     "iopub.status.busy": "2023-12-14T06:31:09.896709Z",
     "iopub.status.idle": "2023-12-14T06:31:10.012517Z",
     "shell.execute_reply": "2023-12-14T06:31:10.011645Z",
     "shell.execute_reply.started": "2023-12-14T06:31:09.897171Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 3/3 [00:00<00:00, 802.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function evaluate.loading.load(path: str, config_name: Union[str, NoneType] = None, module_type: Union[str, NoneType] = None, process_id: int = 0, num_process: int = 1, cache_dir: Union[str, NoneType] = None, experiment_id: Union[str, NoneType] = None, keep_in_memory: bool = False, download_config: Union[datasets.download.download_config.DownloadConfig, NoneType] = None, download_mode: Union[datasets.download.download_manager.DownloadMode, NoneType] = None, revision: Union[str, datasets.utils.version.Version, NoneType] = None, **init_kwargs) -> evaluate.module.EvaluationModule>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = r'./hf/dataset/glue'\n",
    "dataset = load_dataset(dataset_dir, 'cola')\n",
    "# dataset = load_dataset('glue', 'cola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88193e3c-7733-4d7b-a3bb-a9145aba53cc",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-11T12:32:54.756101Z",
     "iopub.status.busy": "2023-12-11T12:32:54.755635Z",
     "iopub.status.idle": "2023-12-11T12:32:54.787159Z",
     "shell.execute_reply": "2023-12-11T12:32:54.786528Z",
     "shell.execute_reply.started": "2023-12-11T12:32:54.756071Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our friends won't buy this analysis, let alone...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One more pseudo generalization and I'm giving up.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One more pseudo generalization or I'm giving up.</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The more we study verbs, the crazier they get.</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Day by day the facts are getting murkier.</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  idx\n",
       "0  Our friends won't buy this analysis, let alone...      1    0\n",
       "1  One more pseudo generalization and I'm giving up.      1    1\n",
       "2   One more pseudo generalization or I'm giving up.      1    2\n",
       "3     The more we study verbs, the crazier they get.      1    3\n",
       "4          Day by day the facts are getting murkier.      1    4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = dataset['train'].with_format(type='pandas')\n",
    "examples[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e6d95f-fcd4-493a-b6ca-d791cf4563da",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-10T12:39:51.421951Z",
     "iopub.status.busy": "2023-12-10T12:39:51.421232Z",
     "iopub.status.idle": "2023-12-10T12:49:57.641504Z",
     "shell.execute_reply": "2023-12-10T12:49:57.640280Z",
     "shell.execute_reply.started": "2023-12-10T12:39:51.421919Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find a module script at /mnt/workspace/glue/glue.py. Module 'glue' doesn't exist on the Hugging Face Hub either.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metric \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mglue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# metric\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/evaluate/loading.py:748\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a [`~evaluate.EvaluationModule`].\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m download_mode \u001b[38;5;241m=\u001b[39m DownloadMode(download_mode \u001b[38;5;129;01mor\u001b[39;00m DownloadMode\u001b[38;5;241m.\u001b[39mREUSE_DATASET_IF_EXISTS)\n\u001b[0;32m--> 748\u001b[0m evaluation_module \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m evaluation_cls \u001b[38;5;241m=\u001b[39m import_main_class(evaluation_module\u001b[38;5;241m.\u001b[39mmodule_path)\n\u001b[1;32m    752\u001b[0m evaluation_instance \u001b[38;5;241m=\u001b[39m evaluation_cls(\n\u001b[1;32m    753\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[1;32m    754\u001b[0m     process_id\u001b[38;5;241m=\u001b[39mprocess_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs,\n\u001b[1;32m    761\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/evaluate/loading.py:681\u001b[0m, in \u001b[0;36mevaluation_module_factory\u001b[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (\u001b[38;5;167;01mConnectionError\u001b[39;00m, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m)):\n\u001b[1;32m    680\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    682\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a module script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    683\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hugging Face Hub either.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    684\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a module script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a module script at /mnt/workspace/glue/glue.py. Module 'glue' doesn't exist on the Hugging Face Hub either."
     ]
    }
   ],
   "source": [
    "metric = load('glue')\n",
    "# metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "257c0f01-6a26-442b-a039-bffc64ac0e11",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-06T07:42:51.652793Z",
     "iopub.status.busy": "2023-12-06T07:42:51.652313Z",
     "iopub.status.idle": "2023-12-06T07:42:52.279718Z",
     "shell.execute_reply": "2023-12-06T07:42:52.279016Z",
     "shell.execute_reply.started": "2023-12-06T07:42:51.652766Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/.cache/modelscope/AI-ModelScope/distilbert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "#模型下载\n",
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "model_dir = snapshot_download('AI-ModelScope/distilbert-base-uncased')\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f9a835f-a72c-442e-81cc-97c3ffe03fdf",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-06T07:43:05.269302Z",
     "iopub.status.busy": "2023-12-06T07:43:05.268814Z",
     "iopub.status.idle": "2023-12-06T07:43:05.324478Z",
     "shell.execute_reply": "2023-12-06T07:43:05.323762Z",
     "shell.execute_reply.started": "2023-12-06T07:43:05.269274Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_dir,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09f87b41-43c7-4d1b-bc12-afa3bb460be5",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-06T07:43:12.789102Z",
     "iopub.status.busy": "2023-12-06T07:43:12.788611Z",
     "iopub.status.idle": "2023-12-06T07:43:12.793147Z",
     "shell.execute_reply": "2023-12-06T07:43:12.792260Z",
     "shell.execute_reply.started": "2023-12-06T07:43:12.789074Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_func(example):\n",
    "    token = tokenizer(example['sentence'],truncation=True)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82af7512-d5a2-4a17-af47-e0bc50f28f0c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-04T12:57:29.387687Z",
     "iopub.status.busy": "2023-12-04T12:57:29.387234Z",
     "iopub.status.idle": "2023-12-04T12:57:29.604489Z",
     "shell.execute_reply": "2023-12-04T12:57:29.603856Z",
     "shell.execute_reply.started": "2023-12-04T12:57:29.387659Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/8551 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e77f116dc26d05b1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e0dda4e16a316c5f.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset_tokenized = dataset.map(preprocess_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f868da0-2e01-41bf-ab2e-5440b64148bf",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-04T12:57:35.358040Z",
     "iopub.status.busy": "2023-12-04T12:57:35.357559Z",
     "iopub.status.idle": "2023-12-04T12:57:35.362942Z",
     "shell.execute_reply": "2023-12-04T12:57:35.362373Z",
     "shell.execute_reply.started": "2023-12-04T12:57:35.358010Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['unacceptable', 'acceptable'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokenized['test'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6436fd12-9c0e-4d74-b943-d982bcdc2a67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T07:43:45.841572Z",
     "iopub.status.busy": "2023-12-06T07:43:45.841102Z",
     "iopub.status.idle": "2023-12-06T07:43:55.276902Z",
     "shell.execute_reply": "2023-12-06T07:43:55.276221Z",
     "shell.execute_reply.started": "2023-12-06T07:43:45.841541Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 15:43:46.219822: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-06 15:43:46.268879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-06 15:43:46.269139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-06 15:43:46.271492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-06 15:43:46.271742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-06 15:43:46.271951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-06 15:43:51.217677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-06 15:43:51.217964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-06 15:43:51.218158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-06 15:43:51.218347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14083 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:08.0, compute capability: 7.0\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: \"Grammar_Wrong\", 1: \"Grammar_Right\"}\n",
    "label2id = {value: key for key, value in id2label.items()}\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    model_dir, num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d1e4a0e-4f30-42e3-b1ce-1cc3ac2d287e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-06T07:44:03.155130Z",
     "iopub.status.busy": "2023-12-06T07:44:03.154655Z",
     "iopub.status.idle": "2023-12-06T07:44:03.175902Z",
     "shell.execute_reply": "2023-12-06T07:44:03.175240Z",
     "shell.execute_reply.started": "2023-12-06T07:44:03.155098Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66955010 (255.41 MB)\n",
      "Trainable params: 66955010 (255.41 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b13e01a0-1982-4fff-9a9a-608dff5fa04e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-04T12:58:01.141750Z",
     "iopub.status.busy": "2023-12-04T12:58:01.141289Z",
     "iopub.status.idle": "2023-12-04T12:58:01.506063Z",
     "shell.execute_reply": "2023-12-04T12:58:01.505353Z",
     "shell.execute_reply.started": "2023-12-04T12:58:01.141725Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    dataset_tokenized[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "tf_validation_dataset = model.prepare_tf_dataset(\n",
    "    dataset_tokenized[\"validation\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c1f970a-e0b1-470f-9445-0d74cf7466d0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-04T13:14:07.551740Z",
     "iopub.status.busy": "2023-12-04T13:14:07.550902Z",
     "iopub.status.idle": "2023-12-04T13:14:07.566952Z",
     "shell.execute_reply": "2023-12-04T13:14:07.566327Z",
     "shell.execute_reply.started": "2023-12-04T13:14:07.551710Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "batch_size = 16\n",
    "num_epochs = 20\n",
    "batches_per_epoch = len(dataset_tokenized[\"train\"]) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps\n",
    ")\n",
    "model.compile(optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37611fbf-8e69-40f9-8937-caa23cd607c7",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-06T07:48:10.527685Z",
     "iopub.status.busy": "2023-12-06T07:48:10.527206Z",
     "iopub.status.idle": "2023-12-06T07:48:10.534305Z",
     "shell.execute_reply": "2023-12-06T07:48:10.533640Z",
     "shell.execute_reply.started": "2023-12-06T07:48:10.527654Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=\"./text_classification_model_save/logs\")\n",
    "\n",
    "checkpoint_filepath = './text_classification_model_save/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "callbacks = [tensorboard_callback,model_checkpoint_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6ff9d11-73fb-4331-9bd5-0f4a14ee4534",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-04T14:26:53.488916Z",
     "iopub.status.busy": "2023-12-04T14:26:53.488443Z",
     "iopub.status.idle": "2023-12-04T14:32:35.677212Z",
     "shell.execute_reply": "2023-12-04T14:32:35.676489Z",
     "shell.execute_reply.started": "2023-12-04T14:26:53.488888Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "534/534 [==============================] - 19s 35ms/step - loss: 0.0057 - accuracy: 0.9975 - val_loss: 1.3801 - val_accuracy: 0.8063\n",
      "Epoch 2/20\n",
      "534/534 [==============================] - 17s 32ms/step - loss: 0.0050 - accuracy: 0.9979 - val_loss: 1.3801 - val_accuracy: 0.8063\n",
      "Epoch 3/20\n",
      "534/534 [==============================] - 17s 32ms/step - loss: 0.0047 - accuracy: 0.9974 - val_loss: 1.3801 - val_accuracy: 0.8063\n",
      "Epoch 4/20\n",
      "534/534 [==============================] - 17s 32ms/step - loss: 0.0041 - accuracy: 0.9981 - val_loss: 1.3801 - val_accuracy: 0.8063\n",
      "Epoch 5/20\n",
      "534/534 [==============================] - 17s 32ms/step - loss: 0.0042 - accuracy: 0.9980 - val_loss: 1.3801 - val_accuracy: 0.8063\n",
      "Epoch 6/20\n",
      "534/534 [==============================] - 17s 32ms/step - loss: 0.0042 - accuracy: 0.9979 - val_loss: 1.3801 - val_accuracy: 0.8063\n",
      "Epoch 7/20\n",
      "534/534 [==============================] - 17s 32ms/step - loss: 0.0054 - accuracy: 0.9975 - val_loss: 1.3801 - val_accuracy: 0.8063\n",
      "Epoch 8/20\n",
      "534/534 [==============================] - 17s 32ms/step - loss: 0.0049 - accuracy: 0.9978 - val_loss: 1.3801 - val_accuracy: 0.8063\n",
      "Epoch 9/20\n",
      "534/534 [==============================] - 17s 32ms/step - loss: 0.0035 - accuracy: 0.9980 - val_loss: 1.3801 - val_accuracy: 0.8063\n",
      "Epoch 10/20\n",
      "534/534 [==============================] - 17s 32ms/step - loss: 0.0042 - accuracy: 0.9980 - val_loss: 1.3801 - val_accuracy: 0.8063\n",
      "Epoch 11/20\n",
      "534/534 [==============================] - 17s 32ms/step - loss: 0.0050 - accuracy: 0.9979 - val_loss: 1.3801 - val_accuracy: 0.8063\n",
      "Epoch 12/20\n",
      "534/534 [==============================] - 17s 32ms/step - loss: 0.0047 - accuracy: 0.9979 - val_loss: 1.3801 - val_accuracy: 0.8063\n",
      "Epoch 13/20\n",
      "534/534 [==============================] - 17s 32ms/step - loss: 0.0059 - accuracy: 0.9973 - val_loss: 1.3801 - val_accuracy: 0.8063\n",
      "Epoch 14/20\n",
      "534/534 [==============================] - 17s 32ms/step - loss: 0.0058 - accuracy: 0.9973 - val_loss: 1.3801 - val_accuracy: 0.8063\n",
      "Epoch 15/20\n",
      "534/534 [==============================] - 17s 32ms/step - loss: 0.0045 - accuracy: 0.9980 - val_loss: 1.3801 - val_accuracy: 0.8063\n",
      "Epoch 16/20\n",
      "534/534 [==============================] - 17s 32ms/step - loss: 0.0051 - accuracy: 0.9975 - val_loss: 1.3801 - val_accuracy: 0.8063\n",
      "Epoch 17/20\n",
      "534/534 [==============================] - 17s 32ms/step - loss: 0.0052 - accuracy: 0.9980 - val_loss: 1.3801 - val_accuracy: 0.8063\n",
      "Epoch 18/20\n",
      "534/534 [==============================] - 17s 32ms/step - loss: 0.0042 - accuracy: 0.9985 - val_loss: 1.3801 - val_accuracy: 0.8063\n",
      "Epoch 19/20\n",
      "534/534 [==============================] - 17s 32ms/step - loss: 0.0057 - accuracy: 0.9977 - val_loss: 1.3801 - val_accuracy: 0.8063\n",
      "Epoch 20/20\n",
      "534/534 [==============================] - 17s 32ms/step - loss: 0.0048 - accuracy: 0.9975 - val_loss: 1.3801 - val_accuracy: 0.8063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fe98058d0d0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs=num_epochs,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ed90423-19c1-41ed-97da-e6549489c4ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T07:48:34.181134Z",
     "iopub.status.busy": "2023-12-06T07:48:34.180666Z",
     "iopub.status.idle": "2023-12-06T07:48:34.859586Z",
     "shell.execute_reply": "2023-12-06T07:48:34.858883Z",
     "shell.execute_reply.started": "2023-12-06T07:48:34.181103Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f0a5c408dc0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调出训练过的checkpoint:\n",
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e1c7e671-cd21-4d6a-ab7f-b920836d3b63",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-06T08:00:53.315313Z",
     "iopub.status.busy": "2023-12-06T08:00:53.314834Z",
     "iopub.status.idle": "2023-12-06T08:00:53.412001Z",
     "shell.execute_reply": "2023-12-06T08:00:53.411273Z",
     "shell.execute_reply.started": "2023-12-06T08:00:53.315286Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_sentence=[\"do u have good one?\",\n",
    "              \"it is the last words\",\n",
    "              \"The judge told that the jurors to think carefully\",\n",
    "              \"I is a good \",\n",
    "              \"I ask him to do job\",]\n",
    "output = model(tokenizer(test_sentence,return_tensors='tf',padding='longest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b2cf4e0-97e0-41f6-9209-8811ca83b249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T08:00:55.062958Z",
     "iopub.status.busy": "2023-12-06T08:00:55.062116Z",
     "iopub.status.idle": "2023-12-06T08:00:55.067752Z",
     "shell.execute_reply": "2023-12-06T08:00:55.067138Z",
     "shell.execute_reply.started": "2023-12-06T08:00:55.062930Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
       "array([[-2.3238587 ,  2.843852  ],\n",
       "       [-3.6311479 ,  4.199532  ],\n",
       "       [ 4.212416  , -3.8671856 ],\n",
       "       [-0.16245975,  0.487365  ],\n",
       "       [ 4.2147355 , -3.8299022 ]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a96880ef-f3b0-4328-b0d8-e11afab539bd",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-06T08:01:29.626471Z",
     "iopub.status.busy": "2023-12-06T08:01:29.625982Z",
     "iopub.status.idle": "2023-12-06T08:01:29.633941Z",
     "shell.execute_reply": "2023-12-06T08:01:29.633297Z",
     "shell.execute_reply.started": "2023-12-06T08:01:29.626443Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Grammar_Right', 'Grammar_Right', 'Grammar_Wrong', 'Grammar_Right', 'Grammar_Wrong']\n"
     ]
    }
   ],
   "source": [
    "classfications = tf.argmax(output.logits, axis=1)\n",
    "\n",
    "classifications = [model.config.id2label[c.numpy()] for c in classfications]\n",
    "print(classifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b8b86b-0bd9-4bc7-aff5-98d2ececcc39",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "source": [
    "## **pytorch training:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cecf6dbe-5a70-4fe0-8dc0-99ae46b538e6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-17T10:34:28.803515Z",
     "iopub.status.busy": "2023-12-17T10:34:28.803069Z",
     "iopub.status.idle": "2023-12-17T10:34:36.199591Z",
     "shell.execute_reply": "2023-12-17T10:34:36.198665Z",
     "shell.execute_reply.started": "2023-12-17T10:34:28.803487Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 18:34:29,667 - modelscope - INFO - PyTorch version 2.0.1+cu118 Found.\n",
      "2023-12-17 18:34:29,670 - modelscope - INFO - TensorFlow version 2.13.0 Found.\n",
      "2023-12-17 18:34:29,671 - modelscope - INFO - Loading ast index from /mnt/workspace/.cache/modelscope/ast_indexer\n",
      "2023-12-17 18:34:29,835 - modelscope - INFO - Loading done! Current index file version is 1.9.5, with md5 a8a545858e67690dd394730b698bbb8b and a total number of 945 components indexed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model下载目录:/mnt/workspace/.cache/modelscope/AI-ModelScope/bert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /mnt/workspace/.cache/modelscope/AI-ModelScope/bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Found cached dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0, 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 下载model, datasets\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
    "import torchmetrics\n",
    "from modelscope import snapshot_download\n",
    "\n",
    "model_dir = snapshot_download('AI-ModelScope/bert-base-cased')\n",
    "print(f\"model下载目录:{model_dir}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir, num_labels=5) # yelp_review_full dataset的类别有5个;\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"./hf/dataset/yelp_review_full\")\n",
    "print(dataset[\"train\"][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ef252dd-be08-4183-bff6-e6ee13fe0984",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-16T08:45:35.825580Z",
     "iopub.status.busy": "2023-12-16T08:45:35.825074Z",
     "iopub.status.idle": "2023-12-16T08:45:35.976202Z",
     "shell.execute_reply": "2023-12-16T08:45:35.975574Z",
     "shell.execute_reply.started": "2023-12-16T08:45:35.825548Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-1b394405ec9298c1_*_of_00008.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-64fa14a8037aa111_*_of_00008.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-6a9eb5bb43e8054b.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-6a9eb5bb43e8054b.arrow\n"
     ]
    }
   ],
   "source": [
    "# dataset process:\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True) \n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    对dataset每个样本,进行truncation;padding;\n",
    "    truncation必须这里完成并且设定max_length;\n",
    "    padding可以在collate_fn中,再padding,实现batch内部padding,而不是整个datasets中padding;\n",
    "    \"\"\"\n",
    "    # return_tensors=None : 返回 list\n",
    "    # return_tensors='py' : 返回 tensor\n",
    "    return tokenizer(examples[\"text\"], padding=False, truncation=True, return_tensors=None,max_length=512) # bert-base-cased模型,参照model card的config.json:  \"max_position_embeddings\": 512, \n",
    "\n",
    "def tokenize_function_batch(examples):\n",
    "    \"\"\"\n",
    "    DataLoader中的collate_fn: 函数中实现: \n",
    "    padding/truncation/dataloder\n",
    "    \"\"\"\n",
    "    texts = [tokenizer(ex[\"text\"], padding=True, truncation=True, max_length=512) for ex in examples]\n",
    "    labels = [ex['label'] for ex in examples]            \n",
    "    return {'label':labels,\n",
    "            'text': texts}\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=False, batch_size=1000, num_proc=8, load_from_cache_file=True) # cpu的逻辑核心可以通过os.cpu_count()获取; batch_size缺省值即为1000,可以省略;\n",
    "train_dataset = tokenized_dataset['train'].shuffle(seed=11).select(range(5000))\n",
    "test_dataset = tokenized_dataset['train'].shuffle(seed=11).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7efc4e33-5ef7-4874-b753-d8edcee2ef5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T08:42:17.793003Z",
     "iopub.status.busy": "2023-12-16T08:42:17.792509Z",
     "iopub.status.idle": "2023-12-16T08:42:17.798148Z",
     "shell.execute_reply": "2023-12-16T08:42:17.797510Z",
     "shell.execute_reply.started": "2023-12-16T08:42:17.792970Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "008f12d3-9a9d-45f0-9b49-40cdd383366b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-16T08:42:21.232355Z",
     "iopub.status.busy": "2023-12-16T08:42:21.231861Z",
     "iopub.status.idle": "2023-12-16T08:42:21.236182Z",
     "shell.execute_reply": "2023-12-16T08:42:21.235437Z",
     "shell.execute_reply.started": "2023-12-16T08:42:21.232321Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_sample_iter = iter(tokenized_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bcd13cbf-64ed-42ee-ac81-8edbb34804c9",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-16T08:42:25.184533Z",
     "iopub.status.busy": "2023-12-16T08:42:25.184030Z",
     "iopub.status.idle": "2023-12-16T08:42:25.190314Z",
     "shell.execute_reply": "2023-12-16T08:42:25.189380Z",
     "shell.execute_reply.started": "2023-12-16T08:42:25.184504Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len:4 ; ('label', 4) \n",
      "\n",
      "len:534 ; ('text', \"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\") \n",
      "\n",
      "len:124 ; ('input_ids', [101, 173, 1197, 119, 2284, 2953, 3272, 1917, 178, 1440, 1111, 1107, 170, 1704, 22351, 119, 1119, 112, 188, 3505, 1105, 3123, 1106, 2037, 1106, 1443, 1217, 10063, 4404, 132, 1119, 112, 188, 1579, 1113, 1159, 1107, 3195, 1117, 4420, 132, 1119, 112, 188, 6559, 1114, 170, 1499, 118, 23555, 2704, 113, 183, 9379, 114, 1134, 1139, 2153, 1138, 3716, 1106, 1143, 1110, 1304, 1696, 1107, 1692, 1380, 5940, 1105, 1128, 1444, 6059, 132, 1105, 1128, 1169, 1243, 5991, 16179, 1106, 1267, 18137, 1443, 1515, 1106, 1267, 1140, 1148, 119, 1541, 117, 1184, 1167, 1202, 1128, 1444, 136, 178, 112, 182, 2807, 1303, 1774, 1106, 1341, 1104, 1251, 11344, 178, 1138, 1164, 1140, 117, 1133, 178, 112, 182, 1541, 4619, 170, 9153, 119, 102]) \n",
      "\n",
      "len:124 ; ('token_type_ids', [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) \n",
      "\n",
      "len:124 ; ('attention_mask', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_sample = next(data_sample_iter)\n",
    "for item in data_sample.items():\n",
    "    if item[0] != 'label':\n",
    "        print(f\"len:{len(item[1])} ; {item} \\n\")\n",
    "    else:\n",
    "        print(f\"len:{item[1]} ; {item} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59bacf9f-0ec8-45fe-8468-61c4aa90c6dc",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-16T08:42:33.352594Z",
     "iopub.status.busy": "2023-12-16T08:42:33.352093Z",
     "iopub.status.idle": "2023-12-16T08:42:33.412722Z",
     "shell.execute_reply": "2023-12-16T08:42:33.411908Z",
     "shell.execute_reply.started": "2023-12-16T08:42:33.352564Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 20777, 17510,  1143,  1118,  1251,  3087,  1128,   112,   173,\n",
      "          1176,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6522, -0.0350,  0.2353,  0.6221, -0.7457]],\n",
      "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "output = model(**encoded_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2b7671-d892-44de-935e-70ef9b8f8e75",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2023-12-16T03:58:06.390370Z",
     "iopub.status.busy": "2023-12-16T03:58:06.389893Z",
     "iopub.status.idle": "2023-12-16T03:58:06.394574Z",
     "shell.execute_reply": "2023-12-16T03:58:06.393713Z",
     "shell.execute_reply.started": "2023-12-16T03:58:06.390342Z"
    },
    "tags": []
   },
   "source": [
    "### <font color=lime size=8  face=kaiti>**pytorch dataset process 心得:**</font>\n",
    "> + torch.utilis.data.DataLoader是torch原生的重要的将Dataset进行Load进入train的重要方法. 其中的collate_fn是每个dataset样本,或者batch的list用于转换的函数,其接收一个batch的list,然后生成一个带有label的字典,\n",
    "> + 这个函数转换的结果再送入DataLoader生成model.forward()所需要的input_ids,其为一个字典,但仅仅保留三个keys:input_ids,token_type_ids,attention_mask,其它key(或者是column)已经被移除;\n",
    "> + transformers.DataCollatorWithPadding,其实就是一个具体的(instanciate)DataLoader()中的collate_fn,只不过,其仅仅完成padding的功能,不对dataset进行任何embedding/tokenizer的动作;所以:\n",
    "> + 所以,在transformers的训练时的wrap后的Trainer类而言, 实现dataset的process可以两个方法: \n",
    "### <font color=lime size=5  face=kaiti>标准的方法是: </font>\n",
    "> 1. 首先datasets.map(tokenizer, num_proc=n)对每个dataset进行tokenizer,此时,必须要truncation=True,max_length=max_length), \n",
    "> 2. 然后transformers.Trainer(data_collator=transformers.DataCollatorWithPadding, dataset=tokenized_dataset);\n",
    "> 3. 缺点是map过程计算会事先将所有的数据都tokenizer一次,非常耗时,虽然有num_proc可以利用CPU的多进程,是不能使用GPU的;\n",
    "### <font color=lime size=5  face=kaiti>或者方法二:</font>\n",
    "> 1. transfomers.Trainer()中的dataset可以送入原始的dataset对象,并不占用多少内存,不事先进行tokenizer;而是将包含tokenizer, padding等等过程全部放入data_collator中;\n",
    "> 2. transformers.Trainer()中的data_collator其实相当于torch.utilis.data.DataLoader原生类中的collate_fn,\n",
    "> 3. 所以,需要自己写一个函数来实现单样本数据或者batch的dataset送入collate_fn,进行tokenizer,padding,truncation等一系列操作,再生成一个字典\n",
    "> 4. 这样, transformers.Trainer()中的dataset, 首先进行collate_fn,进行process,然后,data_collator就会将dataset转换成process后的生成器,并且符合model.forward()的input的格式要求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c4dac9a-c2eb-4eae-bdc0-99fcc779963b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-16T08:35:31.050422Z",
     "iopub.status.busy": "2023-12-16T08:35:31.049942Z",
     "iopub.status.idle": "2023-12-16T08:35:31.054660Z",
     "shell.execute_reply": "2023-12-16T08:35:31.053934Z",
     "shell.execute_reply.started": "2023-12-16T08:35:31.050387Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data = DataLoader(dataset['test'],batch_size=8,shuffle=False, collate_fn=tokenize_function_batch, prefetch_factor=2, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "151d625e-0959-4f57-8f35-8ba08b44f156",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-16T03:58:11.337178Z",
     "iopub.status.busy": "2023-12-16T03:58:11.336676Z",
     "iopub.status.idle": "2023-12-16T03:58:11.348410Z",
     "shell.execute_reply": "2023-12-16T03:58:11.347757Z",
     "shell.execute_reply.started": "2023-12-16T03:58:11.337149Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1573, 119, 6335, 119, 16068, 106, 165, 183, 165, 183, 2240, 160, 6258, 3048, 146, 1180, 1505, 170, 1461, 1120, 1103, 3968, 1104, 170, 6131, 119, 6466, 2213, 146, 1309, 1261, 3267, 8497, 1105, 146, 112, 173, 1930, 1322, 1146, 23993, 6647, 1544, 1103, 4017, 1168, 1190, 1103, 8220, 119, 12998, 1122, 112, 188, 8881, 183, 112, 153, 8043, 1161, 112, 188, 156, 5114, 4184, 119, 165, 183, 165, 183, 1942, 27516, 1110, 170, 1541, 1632, 1282, 1106, 1301, 1165, 1128, 1138, 1149, 1104, 1411, 6065, 1137, 1132, 1702, 1111, 170, 4106, 1282, 1106, 1321, 1800, 1111, 170, 5913, 7978, 1137, 170, 4106, 1480, 1149, 119, 1135, 112, 188, 1198, 1380, 1472, 1106, 1202, 1168, 1190, 171, 5658, 6185, 1122, 1146, 1120, 1103, 6668, 1113, 170, 4306, 1480, 119, 165, 183, 165, 183, 2428, 17059, 1107, 1713, 1128, 112, 1325, 1129, 6573, 1213, 170, 109, 126, 118, 1275, 2267, 4882, 3451, 8898, 1105, 2094, 1128, 1243, 119, 1109, 1363, 1645, 1110, 1103, 2094, 1110, 1909, 1121, 1103, 4309, 2977, 19984, 23418, 119, 1573, 170, 1974, 1104, 1122, 1110, 4382, 3068, 1363, 119, 8696, 1103, 17876, 5298, 1110, 2140, 2785, 1363, 106, 11345, 1736, 1128, 1209, 1129, 6573, 4382, 3945, 8898, 119, 119, 119, 1177, 1104, 1736, 1152, 112, 1231, 1146, 4601, 170, 2113, 119, 1252, 1128, 1202, 2653, 1111, 1103, 2006, 2541, 119, 119, 119, 119, 1177, 1128, 13224, 1243, 1154, 1103, 4840, 1114, 1199, 9494, 106, 165, 183, 165, 183, 1942, 4638, 3713, 1150, 1505, 1132, 12170, 11249, 119, 1247, 112, 188, 123, 3713, 1115, 1209, 1321, 11458, 1105, 1505, 2040, 1113, 1103, 3267, 119, 1220, 2785, 1277, 1505, 23096, 3663, 24162, 15740, 1128, 2367, 119, 6845, 2040, 117, 4932, 1171, 2040, 117, 2147, 2040, 117, 12488, 2040, 117, 3618, 2040, 119, 1192, 1271, 1122, 117, 1152, 1505, 1122, 119, 165, 183, 165, 183, 2107, 1183, 5095, 1226, 1110, 1165, 1152, 6878, 1149, 1106, 1330, 1160, 3713, 119, 1220, 112, 1325, 1155, 1300, 1243, 1113, 2016, 1105, 1505, 1160, 3267, 112, 188, 117, 170, 2092, 1105, 3323, 119, 1220, 1932, 2067, 1149, 1114, 1199, 4106, 4509, 2040, 113, 15276, 117, 12015, 117, 1104, 1736, 106, 114, 119, 165, 183, 165, 183, 1942, 4638, 5913, 1461, 1110, 5536, 1103, 1436, 119, 1220, 6928, 1106, 1128, 119, 119, 119, 119, 1105, 1840, 1128, 1199, 2666, 119, 119, 119, 119, 1114, 4252, 7136, 27230, 119, 1252, 1107, 170, 5733, 3505, 1236, 1104, 1736, 119, 1192, 1198, 1138, 1106, 1301, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data))['text'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42d8866d-0425-4d94-8610-fe7bf404f13d",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2023-12-14T08:39:38.421823Z",
     "iopub.status.busy": "2023-12-14T08:39:38.421325Z",
     "iopub.status.idle": "2023-12-14T08:39:39.516724Z",
     "shell.execute_reply": "2023-12-14T08:39:39.516065Z",
     "shell.execute_reply.started": "2023-12-14T08:39:38.421792Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /mnt/workspace/.cache/modelscope/AI-ModelScope/bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a904eeec-c47f-47e2-9594-5d573414556b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-16T08:42:40.603556Z",
     "iopub.status.busy": "2023-12-16T08:42:40.603065Z",
     "iopub.status.idle": "2023-12-16T08:42:40.609586Z",
     "shell.execute_reply": "2023-12-16T08:42:40.608934Z",
     "shell.execute_reply.started": "2023-12-16T08:42:40.603528Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# metrics 函数:\n",
    "def compute_metrics(p: transformers.EvalPrediction) -> dict:\n",
    "    \"\"\"\n",
    "    torchmetrics类实现类别的precision,recall,f1_score, accuracy:\n",
    "    1. transformers.Trainer()中,生成的transformer.EvalPrediction的输出为ndarray类型,转换为tensor为torchmetrics库使用;\n",
    "    2. 转换成字典;\n",
    "    也可以使用sklearn.metrics.precision_recall_fscore_support函数实现,该函数要求input为ndarray.\n",
    "    \"\"\"\n",
    "    preds,labels=p\n",
    "    # preds = np.argmax(preds, axis=-1)\n",
    "    \n",
    "    preds = torch.from_numpy(preds)\n",
    "    labels= torch.from_numpy(labels)\n",
    "    \n",
    "    precision = torchmetrics.functional.precision(preds,labels,task='multiclass',num_classes=5)\n",
    "    recall = torchmetrics.functional.recall(preds,labels,task='multiclass',num_classes=5)\n",
    "    f1 = torchmetrics.functional.f1_score(preds,labels,task='multiclass',num_classes=5)\n",
    "    accuracy = torchmetrics.functional.accuracy(preds,labels,task='multiclass',num_classes=5)\n",
    "    \n",
    "    # precision, recall, f1, _ = precision_recall_fscore_support(labels.flatten(), preds.flatten(), average='weighted', zero_division=0)\n",
    "   \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ce708ec-3662-421c-8659-5e49e55834de",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-16T08:45:44.937728Z",
     "iopub.status.busy": "2023-12-16T08:45:44.936857Z",
     "iopub.status.idle": "2023-12-16T08:45:44.951844Z",
     "shell.execute_reply": "2023-12-16T08:45:44.951206Z",
     "shell.execute_reply.started": "2023-12-16T08:45:44.937697Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "batch_size=8\n",
    "EarlyStoppingCallBack = transformers.EarlyStoppingCallback(early_stopping_patience=30,)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='test_trainer',\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps = 50,\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps =500,\n",
    "    save_total_limit=1,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    group_by_length=True,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    greater_is_better=True,\n",
    "    dataloader_num_workers=8,\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "collate_fn = transformers.DataCollatorWithPadding(tokenizer=tokenizer,padding=True,max_length=None,return_tensors='pt') # 当tokenizer参数有输入时,data_collator缺省为DataCollatorWithPadding,因而该句可以省略\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn, # 当tokenizer参数有输入时,data_collator缺省为DataCollatorWithPadding,因而该句可以省略\n",
    "    train_dataset=train_dataset,    \n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallBack],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9628aea4-ba51-416e-8f19-608668475cde",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2023-12-16T08:45:47.844039Z",
     "iopub.status.busy": "2023-12-16T08:45:47.843532Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1614' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1614/3125 07:07 < 06:41, 3.77 it/s, Epoch 2.58/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.962969</td>\n",
       "      <td>0.622000</td>\n",
       "      <td>0.622000</td>\n",
       "      <td>0.622000</td>\n",
       "      <td>0.622000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.123250</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.033388</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>0.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.936984</td>\n",
       "      <td>0.598000</td>\n",
       "      <td>0.598000</td>\n",
       "      <td>0.598000</td>\n",
       "      <td>0.598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915111</td>\n",
       "      <td>0.599000</td>\n",
       "      <td>0.599000</td>\n",
       "      <td>0.599000</td>\n",
       "      <td>0.599000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.929415</td>\n",
       "      <td>0.602000</td>\n",
       "      <td>0.602000</td>\n",
       "      <td>0.602000</td>\n",
       "      <td>0.602000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.835709</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.643000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.809572</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.668000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.755530</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.997200</td>\n",
       "      <td>0.748524</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.689000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.997200</td>\n",
       "      <td>0.716751</td>\n",
       "      <td>0.707000</td>\n",
       "      <td>0.707000</td>\n",
       "      <td>0.707000</td>\n",
       "      <td>0.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.997200</td>\n",
       "      <td>0.776680</td>\n",
       "      <td>0.687000</td>\n",
       "      <td>0.687000</td>\n",
       "      <td>0.687000</td>\n",
       "      <td>0.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.997200</td>\n",
       "      <td>0.717483</td>\n",
       "      <td>0.697000</td>\n",
       "      <td>0.697000</td>\n",
       "      <td>0.697000</td>\n",
       "      <td>0.697000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.997200</td>\n",
       "      <td>0.626783</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>0.741000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.997200</td>\n",
       "      <td>0.670406</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.997200</td>\n",
       "      <td>0.575746</td>\n",
       "      <td>0.772000</td>\n",
       "      <td>0.772000</td>\n",
       "      <td>0.772000</td>\n",
       "      <td>0.772000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.997200</td>\n",
       "      <td>0.562821</td>\n",
       "      <td>0.782000</td>\n",
       "      <td>0.782000</td>\n",
       "      <td>0.782000</td>\n",
       "      <td>0.782000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.997200</td>\n",
       "      <td>0.571706</td>\n",
       "      <td>0.777000</td>\n",
       "      <td>0.777000</td>\n",
       "      <td>0.777000</td>\n",
       "      <td>0.777000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.997200</td>\n",
       "      <td>0.525682</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>0.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.783900</td>\n",
       "      <td>0.545208</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.783900</td>\n",
       "      <td>0.495092</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.809000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.783900</td>\n",
       "      <td>0.473918</td>\n",
       "      <td>0.828000</td>\n",
       "      <td>0.828000</td>\n",
       "      <td>0.828000</td>\n",
       "      <td>0.828000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.783900</td>\n",
       "      <td>0.484238</td>\n",
       "      <td>0.827000</td>\n",
       "      <td>0.827000</td>\n",
       "      <td>0.827000</td>\n",
       "      <td>0.827000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.783900</td>\n",
       "      <td>0.441593</td>\n",
       "      <td>0.838000</td>\n",
       "      <td>0.838000</td>\n",
       "      <td>0.838000</td>\n",
       "      <td>0.838000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.783900</td>\n",
       "      <td>0.423298</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>0.852000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.783900</td>\n",
       "      <td>0.426712</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.845000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.783900</td>\n",
       "      <td>0.383941</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.783900</td>\n",
       "      <td>0.399344</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.783900</td>\n",
       "      <td>0.343229</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.587200</td>\n",
       "      <td>0.314921</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.888000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.587200</td>\n",
       "      <td>0.367081</td>\n",
       "      <td>0.858000</td>\n",
       "      <td>0.858000</td>\n",
       "      <td>0.858000</td>\n",
       "      <td>0.858000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.587200</td>\n",
       "      <td>0.337548</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.859000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb67df3-e9e0-4d88-b7a2-b03c21ff7747",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42aff401-86e5-42d0-a58b-9f9ddba51692",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-17T11:20:16.063955Z",
     "iopub.status.busy": "2023-12-17T11:20:16.063473Z",
     "iopub.status.idle": "2023-12-17T11:20:17.266386Z",
     "shell.execute_reply": "2023-12-17T11:20:17.265637Z",
     "shell.execute_reply.started": "2023-12-17T11:20:16.063927Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = 'test_trainer/checkpoint-3000'\n",
    "FT_model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=5)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=True) \n",
    "# FT_pipeline = Pipeline(task='text-classification',model=FT_model, tokenizer=tokenizer,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5053fff1-5a45-4bca-b7e5-9925ced295a7",
   "metadata": {},
   "source": [
    "我们可以将Yelp Review Full数据集中的评分类别解释为以下五个类别：\n",
    "\n",
    "1. 评分为1：非常差评（Very negative）\n",
    "2. 评分为2：差评（Negative）\n",
    "3. 评分为3：中性评价（Neutral）\n",
    "4. 评分为4：好评（Positive）\n",
    "5. 评分为5：非常好评（Very positive）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "549a9d4d-bc9e-4c3b-ac38-2f2e4dcded04",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-17T11:32:46.219495Z",
     "iopub.status.busy": "2023-12-17T11:32:46.219030Z",
     "iopub.status.idle": "2023-12-17T11:32:46.291217Z",
     "shell.execute_reply": "2023-12-17T11:32:46.290399Z",
     "shell.execute_reply.started": "2023-12-17T11:32:46.219467Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1184,  170, 2712, 1873,  106,  106,  106,  106,  106,  117,  146,\n",
      "         1169,  112,  189, 1567, 1128, 1167,  106,  106,  106,  106,  106,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.3139, -2.1106, -2.5306, -1.1107,  5.0567]]), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"what a beautiful girl !!!!!, I can't love you more!!!!!\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "FT_model.eval() # 设置为评估模式,去除dropout\n",
    "with torch.no_grad():\n",
    "    output = FT_model(**encoded_input)\n",
    "print(output)\n",
    "torch.argmax(output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf84bb-08af-4299-9313-2cb6a18e3071",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11c387a-903d-4a48-908d-1c40c70133a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae55b23-0d27-4531-8e3f-377c8a3c8a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
