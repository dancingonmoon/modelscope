import os
import asyncio
from openai import AsyncOpenAI
from openai.types.responses import ResponseTextDeltaEvent
from agents import OpenAIChatCompletionsModel, Agent, Runner, set_default_openai_client, set_tracing_disabled, \
    FileSearchTool
from agents.model_settings import ModelSettings
from rich import print
from rich.markdown import Markdown
import platform
from typing import Literal
import openai


# ç”±äºAgents SDKé»˜è®¤æ”¯æŒçš„æ¨¡å‹æ˜¯OpenAIçš„GPTç³»åˆ—ï¼Œå› æ­¤åœ¨ä¿®æ”¹åº•å±‚æ¨¡å‹çš„æ—¶å€™ï¼Œéœ€è¦å°†external_client è®¾ç½®ä¸ºï¼šset_default_openai_client(external_client)

def custom2default_openai_model(model: str, base_url: str, api_key: str, ):
    custom_client = AsyncOpenAI(base_url=base_url, api_key=api_key)
    set_default_openai_client(custom_client)
    # we disable tracing under the assumption that you don't have an API key
    # from platform.openai.com. If you do have one, you can either set the `OPENAI_API_KEY` env var
    # or call set_tracing_export_api_key() to set a tracing specific key
    set_tracing_disabled(disabled=True)  # ä¸åœ¨platform.openai.comä¸Štrace
    default_openai_model = OpenAIChatCompletionsModel(model=model, openai_client=custom_client)
    return default_openai_model


async def agents_async_chat_once(agent: Agent, input_items: list[dict],
                                 runner_mode: Literal['async', 'stream'] = 'async'):
    """
    è¾“å…¥[{"role": "user", "content": prompt}]æ ¼å¼prompt,è¾“å‡ºagentçš„resultç±»ï¼Œå¯ä»¥é€šè¿‡result.new_itemså±æ€§æ¥æŸ¥çœ‹å…¨éƒ¨çš„äº‹ä»¶ï¼›
    result.new_items[0].raw_itemï¼Œå¯ä»¥çœ‹å…·ä½“çš„å›å¤å†…å®¹ï¼›to_input_list()æ–¹æ³•ï¼Œå¯ä»¥ç›´æ¥å°†ç”¨æˆ·çš„è¾“å…¥å’Œæœ¬æ¬¡è¾“å‡ºç»“æœæ‹¼æ¥æˆä¸€ä¸ªæ¶ˆæ¯åˆ—è¡¨
    :param agent:
    :param input_items: list[dict],è¡¨ç¤ºè¾“å…¥çš„promptæ ¼å¼åˆ—è¡¨ï¼Œä¾‹å¦‚: [{"role": "user", "content": prompt}]
    :param runner_mode:
    :return:
    """
    result = None
    if runner_mode == 'async':
        result = await Runner.run(agent, input_items)
        print(Markdown(result.final_output))
    elif runner_mode == 'stream':
        result = Runner.run_streamed(agent, input_items)
        async for event in result.stream_events():
            if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
                print(event.data.delta, end="", flush=True)
    return result


async def agents_chat_continuous(agent: Agent, runner_mode: Literal['async', 'stream'] = 'async'):
    input_item = []
    while True:
        user_input = input("\nğŸ’¬ è¯·è¾“å…¥ä½ çš„æ¶ˆæ¯(è¾“å…¥quité€€å‡º):")
        if user_input.lower() in ['exit', 'quit']:
            print("âœ… å¯¹è¯å·²ç»“æŸ")
            break
        input_item.append({"role": "user", "content": user_input})
        result = await agents_async_chat_once(agent=agent, input_items=input_item, runner_mode=runner_mode)
        input_item = result.to_input_list()


if __name__ == '__main__':
    # model = 'qwen-plus'
    model = 'qwen-turbo-latest'
    base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
    default_OpenAIModel = custom2default_openai_model(model=model,
                                                      base_url=base_url,
                                                      api_key=os.getenv("DASHSCOPE_API_KEY"),
                                                      )
    agent = Agent(name="my_assistant", instructions="ä½ æ˜¯ä¸€ååŠ©äººä¸ºä¹çš„åŠ©æ‰‹ã€‚",
                  model=default_OpenAIModel,
                  model_settings=ModelSettings(
                      tool_choice='auto',
                      parallel_tool_calls=False,
                      extra_body={
                          # "enable_thinking": True, # only support stream call
                          "enable_search": True,
                          'search_options': {
                              "forced_search": False,  # å¼ºåˆ¶å¼€å¯è”ç½‘æœç´¢
                              "enable_source": False,  # ä½¿è¿”å›ç»“æœåŒ…å«æœç´¢æ¥æºçš„ä¿¡æ¯ï¼ŒOpenAI å…¼å®¹æ–¹å¼æš‚ä¸æ”¯æŒè¿”å›
                              "enable_citation": True,  # å¼€å¯è§’æ ‡æ ‡æ³¨åŠŸèƒ½
                              "citation_format": "[ref_<number>]",  # è§’æ ‡å½¢å¼ä¸º[ref_i]
                              "search_strategy": "pro"  # "pro"æ—¶,æ¨¡å‹å°†æœç´¢10æ¡äº’è”ç½‘ä¿¡æ¯
                          }
                      }
                  ),
                  # tools=[WebSearchTool(user_location={"type": "approximate", "city": "New York"})], # ç›®å‰åªæ”¯æŒopenAIçš„æ¨¡å‹


                  )
    # è®¾ç½®äº‹ä»¶å¾ªç¯ç­–ç•¥
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    # è¿è¡Œä¸»åç¨‹
    asyncio.run(agents_chat_continuous(agent, runner_mode='stream'), debug=False)
    # prompt = "hi"
    # result_items = asyncio.run(agents_async_chat_once(agent, prompt=prompt, runner_mode='async'), debug=False)
    # print(type(result_items))


