import os
import uuid
import operator
import json
from typing import Literal, Union

import typing_extensions
from typing_extensions import TypedDict
from typing import Annotated
from pydantic import BaseModel

from tempfile import TemporaryDirectory
import gradio

from langchain_community import chat_models
from langchain_community.document_loaders import WebBaseLoader, Docx2txtLoader
from langchain_community.agent_toolkits import FileManagementToolkit
from langchain.chat_models import init_chat_model
from langchain_qwq import ChatQwen, ChatQwQ
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.messages import AnyMessage
from langchain_tavily import TavilySearch

from langgraph.graph.message import add_messages
from langgraph.graph import StateGraph, START, END
from langgraph.graph.state import CompiledStateGraph
from langgraph.types import Command

from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.errors import GraphRecursionError

from dataclasses import dataclass

import asyncio


async def web_txtLoader(url: str | list[str] = '',
                        verify_ssl: bool = True):
    #  https://python.langchain.com/docs/how_to/document_loader_web/
    headers = {'User-Agent': 'Mozilla/5.0'}  # è®¾ç½®è¯·æ±‚å¤´,é˜²æ­¢ç½‘ç«™åçˆ¬è™«æœºåˆ¶

    loader = WebBaseLoader(web_path=url, verify_ssl=verify_ssl)
    docs = []
    async for doc in loader.alazy_load():
        docs.append(doc)
    return docs


async def docx_txtLoader(file_path: str | list[str] = None, ):
    #  https://python.langchain.com/docs/integrations/document_loaders/microsoft_word/
    loader = Docx2txtLoader(file_path=file_path, )
    docs = []
    async for doc in loader.alazy_load():
        docs.append(doc)
    return docs


class langchain_qwen_llm:
    def __init__(self,
                 model: str = 'qwen-turbo',
                 base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",  # é˜¿é‡Œäº‘å›½å†…ç«™ç‚¹(é»˜è®¤ä¸ºå›½é™…ç«™ç‚¹),
                 streaming: bool = False,
                 enable_thinking: bool = False,
                 thinking_budget: int = 100,
                 extra_body: dict = None,
                 tools: list = None,
                 structure_output: dict[str, typing_extensions.Any] | BaseModel | type | None = None,
                 system_instruction: str | list[AnyMessage] = "You are a helpful assistant."
                 ):
        """
        langchain-qwqåº“ä¸­çš„ChatQwQä¸ChatQwené’ˆå¯¹Qwen3è¿›è¡Œäº†ä¼˜åŒ–ï¼›ç„¶è€Œï¼Œå…¶ç¼ºçœçš„base_urlå´æ˜¯é˜¿é‡Œäº‘çš„å›½é™…ç«™ç‚¹ï¼›å›½å†…ä½¿ç”¨éœ€è¦æ›´æ”¹base_urlä¸ºå›½å†…ç«™ç‚¹
        :param model: str
        :param base_url: str
        :param streaming: bool
        :param enable_thinking: bool; Qwen3 model only
        :param thinking_budget: int
        :param extra_body: dict; ç¼ºçœ{"enable_search": True}
        :param tools: list
        :param structure_output: TypedDict
        :param system_instruction: str
        """
        if extra_body is None:
            extra_body = {
                "enable_search": True
            }
        self.model = ChatQwen(
            model=model,
            base_url=base_url,
            streaming=streaming,
            enable_thinking=enable_thinking,
            thinking_budget=thinking_budget,
            extra_body=extra_body
        )
        if tools is not None:
            self.model = self.model.bind_tools(tools)

        if structure_output is not None:
            self.model = self.model.with_structured_output(structure_output)

    async def astreamPrint(self, prompt,
                           thread_id: str = None):
        """
        å¼‚æ­¥æµå¼æ‰“å°Agent response
        response json: {'agent':{'messages':[AIMessage(content='',
                                             additional_kwargs={'reasoning_content': 'æ­£åœ¨æ€è€ƒä¸­...'},
                                             response_metadata={'finish_reason','model_name'},
                                             id='',
                                             usage_metadata={},
                                             output_token_details={})]}}
        :param prompt:
        :param thread_id: Short-term memory (thread-level persistence) enables agents to track multi-turn conversations
        :return:
        """
        config = {"configurable": {"thread_id": thread_id}}
        response = self.model.astream(input=prompt, config=config, )

        is_first = True
        is_end = False
        async for msg in response:
            # print(f'response,msg: {msg}')
            if hasattr(msg, 'additional_kwargs') and "reasoning_content" in msg.additional_kwargs:
                if is_first:
                    print("Starting to think...")
                    is_first = False
                    is_end = True
                print(msg.additional_kwargs["reasoning_content"], end="", flush=True)
            if hasattr(msg, 'content') and msg.content:
                if is_end:
                    print("\nThinking ended")
                    is_end = False
                print(msg.content, end="", flush=True)

    async def multi_turn_conversation(self, thread_id: str = None):
        while True:
            try:
                user_input = input("\nUser: ")
                if user_input.lower() in ["quit", "exit", "q"]:
                    print("Goodbye!")
                    break

                await self.astreamPrint(user_input, thread_id=thread_id)
            except Exception as e:
                print(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
                break
            except KeyboardInterrupt:
                print("\nç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºç¨‹åº")
                break


class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]


class langgraph_agent:
    def __init__(self,
                 model: Union[str, chat_models] = 'qwen-turbo',
                 tools: list = None,
                 checkpointer: InMemorySaver | None = None,
                 structure_output: dict[str, typing_extensions.Any] | BaseModel | type | None = None,
                 system_instruction: str | list[AnyMessage] = "You are a helpful assistant."
                 ):
        """
        :param model: str
        :param tools: list
        :param structure_output: TypedDict; requires the model to support `.with_structured_output`
        :param system_instruction: str
        """
        if checkpointer is None:
            checkpointer = InMemorySaver()
        params = dict(
            model=model,
            response_format=structure_output,
            prompt=system_instruction,
            checkpointer=checkpointer)
        if tools is not None:
            params['tools'] = tools
        else:
            params['tools'] = []

        if structure_output is not None:
            params['response_format'] = structure_output

        self.agent = create_react_agent(**params)

    async def astreamOutput(self, input: str | State,
                            stream_modes: Literal['values', 'updates', 'custom', 'messages', 'debug'] | list[
                                Literal['values', 'updates', 'custom', 'messages', 'debug']] = 'updates',
                            thread_id: str = None,
                            print_mode: Literal['token', 'think', 'model_output', 'tools', 'None'] = 'None'):
        """
        å¼‚æ­¥æµå¼æ‰“å°Agent response,å¯ä»¥åŒæ—¶æ¥å—'updates','messages'ä¸¤ç§stream_mode,ä»¥ä¾¿åŒæ—¶stream token,å’Œè¾“å‡ºstructured_response;
        messagesç‰¹ç‚¹: 1) streamè¾“å‡ºllm token,åŒ…æ‹¬reason_content,ä»¥åŠæ¨¡å‹content;
                     2) ä¸èƒ½è¾“å‡ºgenerate_structured_response;
        updatesç‰¹ç‚¹: ä¸æ˜¯tokençº§åˆ«streamè¾“å‡º,æ˜¯æ¯ä¸ªæ­¥éª¤è¾“å‡º.ä¾‹å¦‚,åªè¾“å‡ºä¸¤æ¡update messages:
                        a)æ¨¡å‹è¾“å‡º(å¸¦æ€è€ƒåœ¨ä¸€æ¡messageå†…) b)structured_response
        åƒæ–‡æ¨¡å‹ response json: {'agent':{'messages':[AIMessage(content='',
                                             additional_kwargs={'reasoning_content': 'æ­£åœ¨æ€è€ƒä¸­...'},
                                             response_metadata={'finish_reason','model_name'},
                                             id='',
                                             usage_metadata={},
                                             output_token_details={})]}}
        :param prompt:
        :param stream_modes: str,æˆ–è€…åŒ…å«å¤šä¸ªstream_modeçš„åˆ—è¡¨
        :param thread_id: Short-term memory (thread-level persistence) enables agents to track multi-turn conversations
        :param print_mode:
            'token': to print streaming token_level for "messages"
            'think': to print streaming think
            'tools': to print tools
            'model_output': to print model_output
            'None': no printing
        :return: æ ¹æ®stream_modeè¿”å›ä¸åŒå…ƒç»„ï¼š
            1) if stream_mode = 'updates', return: updates_think_content, updates_modelOutput, updates_finish_reason, structrued_response
            2) if stream_mode = 'messages', return: msg_think_content, msg_modelOutput, msg_finish_reason, structed_response(ä¸ºä¸updateså¯¹åº”ï¼Œæ— æ„ä¹‰)
            3) if stream_mode = ['updates', 'messages'], return ((updates returns), (messages returns))
        """
        if isinstance(stream_modes, str):
            stream_modes = [stream_modes]

        if isinstance(input, str):
            message = {"messages": input}
        else:
            message = input

        config = {"configurable": {"thread_id": thread_id}}
        response = self.agent.astream(input=message, config=config, stream_mode=stream_modes)
        # response json: {'agent':{'messages':[AIMessage(content='',
        #                                      additional_kwargs={'reasoning_content': 'æ­£åœ¨æ€è€ƒä¸­...'},
        #                                      response_metadata={'finish_reason','model_name'},
        #                                      id='',
        #                                      usage_metadata={},
        #                                      output_token_details={})

        isFirst_updates_think = True
        isFirst_updates_modelOutput = True
        isFirst_updates_toolCalls = True
        isFirst_msg_think = True
        isFirst_msg_toolCalls = True
        isFirst_msg_modelOutput = True
        updates_think_content = ""
        updates_modelOutput = ""
        updates_finish_reason = ""
        structured_response = {}

        msg_think_content = ""
        msg_modelOutput = ""
        msg_finish_reason = ""

        async for stream_mode, msg in response:
            # print(f'response: {msg}')
            if stream_mode == 'updates':
                agent_msgs = []
                tool_msgs = []
                # å¤„ç†èŠ‚ç‚¹(agent)æ¶ˆæ¯
                if 'agent' in msg:
                    if 'messages' in msg['agent']:
                        agent_msgs = msg['agent']["messages"]  # æ¶ˆæ¯åˆ—è¡¨
                    for agent_msg in agent_msgs:
                        #  è¾“å‡ºreasoning:
                        if hasattr(agent_msg,
                                   'additional_kwargs') and "reasoning_content" in agent_msg.additional_kwargs:
                            if isFirst_updates_think:
                                print("\n*Starting to think...*  \n")
                                isFirst_updates_think = False
                            updates_think_content = agent_msg.additional_kwargs["reasoning_content"]
                            if 'think' in print_mode and 'token' not in print_mode:
                                print(updates_think_content, end="", flush=True)
                        #  è¾“å‡ºcontent:
                        if hasattr(agent_msg, 'content') and agent_msg.content:
                            if isFirst_updates_modelOutput:
                                print("\n*Starting model output...*  \n")
                                isFirst_updates_modelOutput = False
                            updates_modelOutput = agent_msg.content
                            if 'model_output' in print_mode and 'token' not in print_mode:
                                print(updates_modelOutput, end="", flush=True)
                        if hasattr(agent_msg, 'response_metadata'):
                            updates_finish_reason = agent_msg.response_metadata.get('finish_reason', None)
                            if updates_finish_reason == "stop":
                                print("\n*Ending model output...*  \n")
                                isFirst_updates_modelOutput = True
                            if updates_finish_reason == "tool_calls":
                                print("\n*Ending tool calls...*  \n")
                                isFirst_updates_toolCalls = True
                # å¤„ç†toolsæ¶ˆæ¯
                if 'tools' in msg:
                    if 'messages' in msg['tools']:
                        tool_msgs = msg['tools']["messages"]  # æ¶ˆæ¯åˆ—è¡¨
                    for tool_msg in tool_msgs:
                        if hasattr(tool_msg, 'name'):
                            if isFirst_updates_toolCalls:
                                print(f"tool: {tool_msg.name}, invoked ")
                                isFirst_updates_toolCalls = False
                        if hasattr(tool_msg, 'content') and tool_msg.content:
                            if 'tools' in print_mode:
                                print(tool_msg.content, end="", flush=True)

                # å¤„ç†{generate_structured_response:{'structured_response': None}}æ¶ˆæ¯:
                if 'generate_structured_response' in msg:
                    if 'structured_response' in msg['generate_structured_response']:
                        structured_response = msg['generate_structured_response']['structured_response']
                        # print(f"\nstructured_response: {structured_response}\n")
                    else:
                        print(f"\nagentæ²¡æœ‰ç”Ÿæˆstructured_response\n")

            if stream_mode == 'messages':
                llm_token, metadata = msg
                # print(f"llm_token:{llm_token}")
                # print(f"metadata: {metadata}")
                # è¾“å‡ºreasoning:
                if hasattr(llm_token, "additional_kwargs"):
                    add_kwargs = llm_token.additional_kwargs
                    if 'reasoning_content' in add_kwargs:
                        msg_think_content = add_kwargs['reasoning_content']
                        if isFirst_msg_think:
                            print("\n*Start Thinking...*  \n")
                            isFirst_msg_think = False
                        if 'think' in print_mode:
                            print(msg_think_content, end="", flush=True)
                    if 'tool_calls' in add_kwargs:
                        if isFirst_msg_toolCalls:
                            print("\n*Start tools_call...*  \n")
                            isFirst_msg_toolCalls = False
                        for dict in add_kwargs['tool_calls']:
                            if 'function' in dict:
                                if 'arguments' in dict['function']:
                                    if 'tools' in print_mode:
                                        print(dict['function']['arguments'], end="", flush=True)

                    if hasattr(llm_token, "response_metadata"):
                        resp_metadata = llm_token.response_metadata
                        msg_finish_reason = resp_metadata.get('finish_reason', None)
                        if msg_finish_reason == "stop":
                            print("\n*Ending model output...*  \n")
                            isFirst_msg_modelOutput = True
                        if msg_finish_reason == "tool_calls":
                            print("\n*Ending tool_calls...*  \n")

                # è¾“å‡ºcontent:
                if hasattr(llm_token, 'content'):
                    if llm_token.content:
                        msg_modelOutput = llm_token.content
                        if isFirst_msg_modelOutput:
                            print("\n*Starting model output...*  \n")
                            isFirst_msg_modelOutput = False
                        if 'model_output' in print_mode:
                            print(msg_modelOutput, end="", flush=True)
                        print(msg_modelOutput, end="", flush=True)

            if 'updates' in stream_modes and 'messages ' not in stream_modes:
                yield updates_think_content, updates_modelOutput, updates_finish_reason, structured_response
            if 'updates' not in stream_modes and 'messages ' in stream_modes:
                yield msg_think_content, msg_modelOutput, msg_finish_reason, structured_response
            if 'updates' in stream_modes and 'messages' in stream_modes:
                yield (updates_think_content, updates_modelOutput, updates_finish_reason, structured_response), (
                    msg_think_content, msg_modelOutput, msg_finish_reason)

    async def multi_turn_conversation(self, stream_modes: Literal[
                                                              'values', 'updates', 'custom', 'messages', 'debug'] |
                                                          list[
                                                              Literal[
                                                                  'values', 'updates', 'custom', 'messages', 'debug']] = 'updates',
                                      print_mode: Literal['token', 'think', 'model_output', 'tools', 'None'] = 'None',
                                      thread_id: str | None = None):
        """
        å¤šè½®å¯¹è¯(ä¼¼ä¹ä¸è®¾ç½®thread_idæ—¶ï¼Œä¹Ÿæ˜¯å…·å¤‡ä¼šè¯çš„è®°å¿†)
        :param stream_modes: Literal['values', 'updates', 'custom', 'messages', 'debug']
        :param thread_id: Short-term memory (thread-level persistence) enables agents to track multi-turn conversations
        :return:
        """
        if isinstance(stream_modes, str):
            stream_modes = [stream_modes]
        while True:
            try:
                user_input = input("\nUser: ")
                if user_input.lower() in ["quit", "exit", "q"]:
                    print("Goodbye!")
                    break
                # ä»¥ä¸‹å¾…å®Œæˆï¼šæ ¹æ®print_modeå‚æ•°ï¼Œprintä¸åŒå†…å®¹
                response = self.astreamOutput(user_input, stream_modes=stream_modes,
                                              print_mode=print_mode, thread_id=thread_id)

                if 'updates' in stream_modes and 'messages ' not in stream_modes:
                    async for updates_think_content, updates_modelOutput, updates_finish_reason, structured_response in response:
                        print(f"")
                if 'updates' not in stream_modes and 'messages ' in stream_modes:
                    msg_think_content, msg_modelOutput, msg_finish_reason, structured_response = response
                    async for msg_think_content, msg_modelOutput, msg_finish_reason, structured_response in response:
                        print(f"")
                if 'updates' in stream_modes and 'messages' in stream_modes:
                    async for (
                            updates_think_content, updates_modelOutput, updates_finish_reason, structured_response), (
                            msg_think_content, msg_modelOutput, msg_finish_reason) in response:
                        print(f"")


            except Exception as e:
                print(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
                break
            except KeyboardInterrupt:
                print("\nç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºç¨‹åº")
                break


class nodeloopState(State):
    loop_count: Annotated[int, operator.add]


class QwenML_trasOptions(BaseModel):
    response: list[AnyMessage] | str = ""  # Qwen_MLæ¨¡å‹å°±éç¿»è¯‘è¯·æ±‚çš„å“åº” = â€œâ€
    text: str = ""  # å¾…ç¿»è¯‘çš„æ–‡æœ¬
    source_lang: str = "auto"  # "Chinese"
    target_lang: str = ""  # "English"
    domains: str = ""  # ç¿»è¯‘çš„é£æ ¼å…·å¤‡æŸé¢†åŸŸçš„ç‰¹æ€§ï¼Œè‡ªç„¶è¯­è¨€(è‹±æ–‡)æè¿°
    translate_request: bool = False  # è¡¨æ˜è¾“å…¥æ˜¯å¦æ˜¯å…³äºè¯­è¨€ç¿»è¯‘çš„è¯·æ±‚ï¼Œå¦‚Trueï¼Œåˆ™ç»“æŸå¯¹è¯
    img: bool = False  # æ˜¯å¦promptä¸­å¸¦æœ‰å›¾ç‰‡ï¼Œhandoffè‡³Qwen_VL


@dataclass
class EvaluationFeedback:
    feedback: str
    score: Literal["pass", "needs_improvement", "end"]



# graph_builder = StateGraph(State)

# Initialize Tavily Search Tool
# https://python.langchain.com/docs/integrations/tools/tavily_search/
# tavily_search_tool = TavilySearch(
#     max_results=5,
#     topic="general",
#     # include_answer=False,
#     # include_raw_content=False,
#     # include_images=False,
#     # include_image_descriptions=False,
#     # search_depth="basic",
#     # time_range="day",
#     # include_domains=None,
#     # exclude_domains=None
# )
# result = tavily_search_tool.invoke(input="ä»Šæ—¥å›½é™…æ–°é—»3æ¡")
# result.keys:dict_keys(['query', 'follow_up_questions', 'answer', 'images', 'results', 'response_time'])
# result.results[0].keys:dict_keys(['url', 'title', 'content', 'score', 'raw_content'])

# # We'll make a temporary directory to avoid clutter
# working_directory = TemporaryDirectory(dir='.')
# LocalFileSystem = FileManagementToolkit(
#     root_dir=str(working_directory.name),  # pass the temporary directory in as a root directory as a workspace
#     # # [CopyFileTool, DeleteFileTool, FileSearchTool, MoveFileTool, ReadFileTool, WriteFileTool, ListDirectoryTool]
#     selected_tools=["read_file", "write_file", "list_directory"],
# ).get_tools()
# print(f"LocalFileSystemç›®å½•: {LocalFileSystem}")


def QwenML_transOption_node(state: State) -> Command[Literal['Qwen_ML_node', 'Qwen_VL_agent']]:
    print(f"+ **prompté¢„åˆ†æ:**")
    response = QwenML_transOption_agent.agent.invoke(input=state, config=config)
    if response['structured_response'] is None:  # æœ‰æ—¶å€™ï¼Œç”±äºpromptå¯¹äºç»“æ„åŒ–è¾“å‡ºçš„ç±»çš„å„ä¸ªitemç”Ÿæˆæœ‰é—æ¼ï¼Œä¼šå¯¼è‡´structure_response=None
        structured_response = json.loads(response['messages'][-1].content)
    else:
        structured_response = response['structured_response']
        if isinstance(structured_response, BaseModel):
            structured_response = structured_response.model_dump()  # å°†BaseModelå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸,ä¸ºä½¿å¾—Pydanticç±»å‹æ¥å—getæ–¹æ³•
    update_state = {'messages': AIMessage(structured_response.get('response', 'no response is available'))}
    if isinstance(structured_response, dict):
        if structured_response.get('translate_request', False):
            goto = "Qwen_ML_node"
            # æ„é€ ä¸€ä¸ªåŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µçš„æ–°çŠ¶æ€
            update_state = QwenML_trasOptions(
                response=structured_response.get('response', 'no response is available'),
                text=structured_response.get('text', ''),
                source_lang=structured_response.get('source_lang', 'auto'),
                target_lang=structured_response.get('target_lang', ''),
                domains=structured_response.get('domains', ''),
                translate_request=structured_response.get('translate_request', False),
                img=structured_response.get('img', False),
            )
            # æœ¬èŠ‚ç‚¹å³gotoï¼Œ ä¹Ÿupdate
            command_params = {'update': update_state, 'goto': goto}
        else:  # æœ¬èŠ‚ç‚¹ï¼Œä¸å¸¦æœ‰gotoå‚æ•°ï¼Œä»…update; commandåï¼Œä¸å†handoff,è‡ªè¡Œç»“æŸ;
            command_params = {'update': update_state}
        if structured_response.get('img', False):  # æœ¬èŠ‚ç‚¹ï¼Œä¸å¸¦æœ‰updateå‚æ•°ï¼Œä»…goto; commandåï¼Œä»…handoff, ä¸æ›´æ–°state;
            goto = "Qwen_VL_agent"
            command_params = {'goto': goto}
            # command_params['update'] = state  # å¦‚æœéœ€è¦è¯†å›¾ï¼Œæ— éœ€update
    else:  # æœ¬èŠ‚ç‚¹ï¼Œä¸å¸¦æœ‰gotoå‚æ•°ï¼Œä»…update; commandåï¼Œä¸å†handoff,è‡ªè¡Œç»“æŸ;
        command_params = {'update': update_state}
    return Command(**command_params)


def Qwen_ML_node(state: QwenML_trasOptions) -> Command[Literal['evaluator']]:
    print(f"+ **è¿›å…¥Qwen_ML_nodeï¼Œå¯åŠ¨é¦–æ¬¡ç¿»è¯‘:**")
    if not state.translate_request:  # æœ¬èŠ‚ç‚¹ï¼Œä¸å¸¦æœ‰gotoå‚æ•°ï¼Œä»…update; commandåï¼Œä¸å†handoff,è‡ªè¡Œç»“æŸ;
        command_params = {'update': {"messages": [AIMessage(content=state.response)]}}
    else:
        text, source_lang, target_lang, domains = state.text, state.source_lang, state.target_lang, state.domains
        extra_body = {
            "translation_options": {
                "source_lang": source_lang,
                "target_lang": target_lang,
                "domains": domains,
            }
        }
        Qwen_MT = langchain_qwen_llm(model='qwen-mt-plus',
                                     streaming=False,
                                     extra_body=extra_body, )
        message = [{"role": "user", "content": text}]
        try:
            response = Qwen_MT.model.invoke(input=message, config=config)
        except Exception as e:
            response = str(e)

        # print(f"**é¦–æ¬¡ç¿»è¯‘:**\n  {response.content}")
        update = {"messages": [AIMessage(content=response.content)],
                  "loop_count": 1,  # operator.addä¼šè‡ªåŠ¨å¢åŠ 1,åˆ†åˆ«ç»Ÿè®¡å¾ªç¯æ¬¡æ•°
                  }
        command_params = {'update': update, 'goto': 'evaluator'}
    return Command(**command_params)


async def evaluator_node(state: nodeloopState) -> Command[Literal['translator']]:
    loop_account = state.get("loop_count", 0)
    print(f"+ **è¿›å…¥ç¿»è¯‘è¯„ä¼°é˜¶æ®µ, å½“å‰ç¬¬{loop_account}æ¬¡ç¿»è¯‘è¯„ä¼°**")
    response = evaluator.astreamOutput(input=state, stream_modes='updates', print_mode='None', thread_id=thread_id)
    command_params = {'update': [AIMessage("agent doesn't generate structured_response, exit!")]}
    async for think, modelOutput, finish_reason, structured_response in response:
        if structured_response is not None and structured_response:  # éç©º{},éNone
            score = structured_response['score']
            feedback = structured_response['feedback']
            if score == 'end' or feedback == 'pass':
                update = {"messages": [AIMessage(feedback)],
                          }  # èŠ‚ç‚¹ç»“æŸæ—¶ï¼Œstateè¾“å‡ºç»“æœæ— éœ€loop_count; stateä¸input stateç›¸åŒ;
                command_params = {'update': update}  # èŠ‚ç‚¹æ— goto, commandåï¼Œä¸å†handoff,è‡ªè¡Œç»“æŸ;
            elif score == "needs_improvement":
                goto = "translator"
                update = {"messages": [HumanMessage(feedback)],
                          'loop_count': 0}  # operator.addä¼šè‡ªåŠ¨å¢åŠ 0,ç»Ÿè®¡å¾ªç¯æ¬¡æ•°;(è¯„ä¼°æ¬¡æ•°ä¸ºç¿»è¯‘æ¬¡æ•°ç›¸ç­‰,å¢åŠ 0)
                command_params = {'goto': goto, 'update': update}

            print(f"**è¯„ä¼°score: {score}**")
            # print(f"**è¯„ä¼°feedback:\n  {response['structured_response']['feedback']}")
        yield Command(**command_params)


async def translator_node(state: nodeloopState) -> Command[Literal['evaluator']]:
    loop_account = state.get("loop_count", 0)
    print(f"+ **è¿›å…¥ç¿»è¯‘æ”¹è¿›é˜¶æ®µ, å½“å‰ç¬¬{loop_account}æ¬¡æ”¹è¿›**")
    response = translator.astreamOutput(input=state, stream_modes='updates', print_mode='None', thread_id=thread_id)
    async for think, modelOutput, finish_reason, structured_response in response:
        if modelOutput:
            update = {"messages": [AIMessage(modelOutput)],
                      "loop_count": 1,  # operator.addä¼šè‡ªåŠ¨å¢åŠ 1,ç»Ÿè®¡å¾ªç¯æ¬¡æ•°
                      }
            # print(f"**ç¿»è¯‘æ”¹è¿›:**\n  {modelOutput}")
            yield Command(
                goto='evaluator',
                update=update, )

        # if think:  # ä¸ºgradio åœ¨UIä¸Šthinkæ¡†å•ç‹¬æ˜¾ç¤ºæ€è€ƒå†…å®¹
        #     if finish_reason:
        #         log = f"End translator output"
        #     else:
        #         log = f"è¿›å…¥ç¿»è¯‘é˜¶æ®µ,å½“å‰ç¬¬{loop_account}æ¬¡æ”¹è¿›"
        #     update = {"messages": [{
        #         "role": "assistant",
        #         "content": think,
        #         "metadata": {"title": "ğŸ§  Thinking",
        #                      "log": log,
        #                      "status": "done"}}], }
        #     yield update  # å¦‚æœyeild command å°†å¯¼è‡´æ€è€ƒéƒ¨åˆ†çš„contentï¼Œä¼šè¢«update,
        #     # ç„¶åï¼Œå†æœªå‡ºç°modeloutputä¹‹å‰ï¼Œæ€è€ƒéƒ¨åˆ†çš„contentï¼Œä¼šè¢«è¦†ç›–å°±ä¼šè¢«é€å…¥evaluator_node,å¯¼è‡´å¾…è¯„ä¼°å†…å®¹ä¸å¤Ÿ


async def graph_astream(graph: StateGraph | CompiledStateGraph, state: State,
                        stream_mode: Literal['messages', 'updates'] = "updates",
                        config: dict = None):
    if isinstance(stream_mode, str):
        stream_mode = [stream_mode]
    try:
        async for stream_mode, chunk in graph.astream(state,
                                                      stream_mode=stream_mode,
                                                      config=config):
            if stream_mode == "messages":
                token, metadata = chunk
                # print(f"graphè¿è¡ŒèŠ‚ç‚¹: {metadata['langgraph_node']}")
                if token.content:
                    print(token.content, end="", flush=True)

            if stream_mode == "updates":
                # print(f"graphå½“å‰updateäº†node: {[*chunk.keys()]}")
                for node in chunk.keys():
                    print(f"graphå½“å‰updateçš„node: {node}")
                    # print(f"å…¶Stateä¸º:{chunk[node]}")
                    #  print modeloutput.content
                    if isinstance(chunk[node], dict):
                        if 'messages' in chunk[node]:
                            modeloutput = chunk[node]['messages']  # list
                            for msg in modeloutput:
                                content = None
                                if hasattr(msg, 'content'):
                                    content = msg.content
                                if isinstance(msg, dict):
                                    content = msg.get('content', None)
                                if content:
                                    print(content)

        print(f"graph: {graph.name} æ­£å¸¸å®Œæˆ !")

    except GraphRecursionError:
        response = "Recursion Error"
        print(f"graph: {graph.name} å“åº”é”™è¯¯:{response} !")


def translation_graph(State: TypedDict, name="translation_graph", checkpointer: None | bool | InMemorySaver = None):
    builder = StateGraph(State, )
    builder.add_node("QwenML_transOption_node", QwenML_transOption_node)
    builder.add_node("Qwen_VL_agent", Qwen_VL_agent.agent)
    builder.add_node("Qwen_ML_node", Qwen_ML_node)
    builder.add_node("evaluator", evaluator_node)
    builder.add_node("translator", translator_node)

    builder.add_edge(START, 'QwenML_transOption_node')
    builder.add_edge("Qwen_VL_agent", "Qwen_ML_node")

    translation_agent = builder.compile(name=name, checkpointer=checkpointer)
    # thread_id = uuid.uuid4()  # 128 ä½çš„éšæœºæ•°ï¼Œé€šå¸¸ç”¨ 32 ä¸ªåå…­è¿›åˆ¶æ•°å­—è¡¨ç¤º
    # config = {"configurable": {"thread_id": thread_id},
    #           "recursion_limit": 20}

    # graph_png_path = r"./translation_agent_graph.png"
    # translation_agent.get_graph().draw_mermaid_png(output_file_path=graph_png_path,)

    return translation_agent


if __name__ == '__main__':
    Qwen_plus = langchain_qwen_llm(model="qwen-plus-latest", enable_thinking=True, streaming=True)
    Qwen_turbo_noThink = langchain_qwen_llm(model="qwen-turbo", )
    Qwen_turbo_noThink_structureOutput = langchain_qwen_llm(model="qwen-turbo",
                                                            structure_output=QwenML_trasOptions)
    Qwen_VL = langchain_qwen_llm(model="qwen-vl-ocr-latest", )
    checkpointer = InMemorySaver()
    QwenML_transOption_agent = langgraph_agent(model=Qwen_turbo_noThink.model,
                                               checkpointer=checkpointer,
                                               structure_output=QwenML_trasOptions,
                                               system_instruction="""
                                               ä½ æ˜¯ä¸€ä¸ªä¼˜ç§€çš„åŠ©æ‰‹ã€‚ä½ å¯¹æ¥æ”¶çš„promptè¿›è¡Œåˆ†æï¼Œå¹¶æŒ‰ç…§å¦‚ä¸‹æŒ‡ç¤º,å°†ç»“æœæŒ‰ç…§structured_responseäº‹å…ˆå®šä¹‰çš„Pydanticç±»ï¼Œè¿›è¡Œç»“æ„åŒ–è¾“å‡ºæ¯ä¸ªå­—æ®µï¼š
                                               1) é¦–å…ˆå¯¹promptä¸­æ˜¯å¦åŒ…å«å›¾ç‰‡ï¼Œåšå‡ºåˆ¤æ–­ï¼Œå¦‚æœpromptåŒ…å«æœ‰å›¾ç‰‡ï¼Œåˆ™ç»“æ„åŒ–è¾“å‡ºå­—æ®µ: img=True,å¦åˆ™ï¼Œimg=False;
                                               2) å†å¯¹promptæ˜¯å¦åŒ…å«æœ‰è¯­è¨€ç¿»è¯‘çš„è¯·æ±‚ï¼Œåšå‡ºåˆ¤æ–­ï¼Œå¦‚æœåŒ…å«æœ‰è¯­è¨€ç¿»è¯‘è¯·æ±‚ï¼Œç»“æ„åŒ–è¾“å‡ºå­—æ®µï¼štranslate_request=Trueï¼›å¦åˆ™,translate_request=False;
                                               3) å¦‚æœpromptåŒ…å«æœ‰è¯­è¨€ç¿»è¯‘çš„è¯·æ±‚ï¼Œè¯·ä»ä¸­æ•´ç†å‡ºå¾…ç¿»è¯‘çš„æ–‡æœ¬ï¼Œå¹¶ä½¿ç”¨ä¸€æ®µè‡ªç„¶è‹±æ–‡(å¿…é¡»ä¸ºè‹±æ–‡)æ€»ç»“ä¸‹å¾…ç¿»è¯‘æ–‡æœ¬çš„é¢†åŸŸï¼Œè¯­æ°”ï¼Œä»è€Œä½¿å¾—ç¿»è¯‘çš„é£æ ¼æ›´ç¬¦åˆæŸä¸ªé¢†åŸŸçš„ç‰¹æ€§ï¼›å¹¶å°†è¯¥æ€»ç»“è¾“å‡ºè‡³ç»“æ„åŒ–è¾“å‡ºå­—æ®µdomains;å¦åˆ™,domains="";
                                               4) å¦‚æœpromptåŒ…å«æœ‰è¯­è¨€ç¿»è¯‘çš„è¯·æ±‚ï¼Œè¯·ä»ä¸­åˆ†æå‡ºç¿»è¯‘è¯·æ±‚çš„æºè¯­è¨€ï¼Œç›®æ ‡è¯­è¨€ï¼Œä»¥åŠæ•´ç†å‡ºçš„å¾…ç¿»è¯‘çš„æ–‡æœ¬ï¼Œå¹¶åˆ†åˆ«ç»“æ„åŒ–è¾“å‡ºåˆ°å­—æ®µ source_lang, target_lang, text; å¦åˆ™, source_lang='auto', target_lang='', text='';
                                               5) å¦‚æœpromptåŒ…å«æœ‰è¯­è¨€ç¿»è¯‘çš„è¯·æ±‚ï¼Œè¯·ç»“æ„åŒ–è¾“å‡ºå­—æ®µï¼šresponse=''; å¦åˆ™ï¼Œå°½ä½ æ‰€èƒ½ï¼Œè¿›è¡Œå›ç­”é—®é¢˜æˆ–è€…æä¾›å¸®åŠ©ï¼Œå¹¶å°†å“åº”å†…å®¹ç»“æ„åŒ–è¾“å‡ºåˆ°response;
                                               6) äº‹å…ˆå®šä¹‰çš„ç”¨äºç»“æ„åŒ–è¾“å‡ºPydanticçš„å„ä¸ªå­—æ®µ(field)ä¸­ï¼Œå¦‚æœä»¥ä¸ŠæŒ‡ç¤ºä¸­æœ‰é—æ¼ï¼Œè¯·ä½¿ç”¨é»˜è®¤å€¼ï¼Œæœ€åå®Œæ•´ç»“æ„åŒ–è¾“å‡ºè¯¥äº‹å…ˆå®šä¹‰çš„Pydanticç±»ã€‚
                                               """)

    Qwen_VL_agent = langgraph_agent(model=Qwen_VL.model,
                                    checkpointer=checkpointer,
                                    structure_output=QwenML_trasOptions,
                                    system_instruction="""
                                    ä½ æ¥æ”¶åˆ°çš„promptå°†åŒæ—¶åŒ…æ‹¬textæ–‡æœ¬,ä»¥åŠå›¾ç‰‡imgæˆ–è€…æ–‡ä»¶file.ä½ å–„äºè¯†å›¾ç†è§£ï¼Œè¯·è¯†å›¾å¹¶ç†è§£è¾“å…¥çš„å›¾ç‰‡æˆ–æ–‡ä»¶ï¼Œè·å–å…¶å…¨éƒ¨å†…å®¹ï¼Œå¹¶ä¸”ç»“åˆæ¥æ”¶çš„prompt,æŒ‰ç…§structured_responseäº‹å…ˆå®šä¹‰çš„Pydanticç±»ï¼Œè¿›è¡Œç»“æ„åŒ–è¾“å‡ºæ¯ä¸ªå­—æ®µï¼š
                                               1) æ˜¾ç„¶ï¼Œä½ å·²ç»æ”¶åˆ°äº†å›¾ç‰‡imgæˆ–è€…æ–‡ä»¶file; è¯·ç»“æ„åŒ–è¾“å‡ºå­—æ®µ: img=True;
                                               2) è¯·å¯¹promptæ˜¯å¦åŒ…å«æœ‰è¯­è¨€ç¿»è¯‘çš„è¯·æ±‚ï¼Œåšå‡ºåˆ¤æ–­ï¼Œå¦‚æœåŒ…å«æœ‰è¯­è¨€ç¿»è¯‘è¯·æ±‚ï¼Œç»“æ„åŒ–è¾“å‡ºå­—æ®µï¼štranslate_request=Trueï¼›å¦åˆ™,translate_request=False;
                                               3) å¦‚æœpromptåŒ…å«æœ‰è¯­è¨€ç¿»è¯‘çš„è¯·æ±‚ï¼Œç»“åˆè¯†å›¾ç†è§£çš„å†…å®¹ï¼Œè¯·ä»ä¸­æ•´ç†å‡ºå¾…ç¿»è¯‘çš„æ–‡æœ¬ï¼Œå¹¶ä½¿ç”¨ä¸€æ®µè‡ªç„¶è‹±æ–‡(å¿…é¡»ä¸ºè‹±æ–‡)æ€»ç»“ä¸‹å¾…ç¿»è¯‘æ–‡æœ¬çš„é¢†åŸŸï¼Œè¯­æ°”ï¼Œä»è€Œä½¿å¾—ç¿»è¯‘çš„é£æ ¼æ›´ç¬¦åˆæŸä¸ªé¢†åŸŸçš„ç‰¹æ€§ï¼›å¹¶å°†è¯¥æ€»ç»“è¾“å‡ºè‡³ç»“æ„åŒ–è¾“å‡ºå­—æ®µdomains;å¦åˆ™,domains="";
                                               4) å¦‚æœpromptåŒ…å«æœ‰è¯­è¨€ç¿»è¯‘çš„è¯·æ±‚ï¼Œç»“åˆè¯†å›¾ç†è§£çš„å†…å®¹ï¼Œè¯·ä»ä¸­åˆ†æå‡ºç¿»è¯‘è¯·æ±‚çš„æºè¯­è¨€ï¼Œç›®æ ‡è¯­è¨€ï¼Œä»¥åŠæ•´ç†å‡ºçš„å¾…ç¿»è¯‘çš„æ–‡æœ¬ï¼Œå¹¶åˆ†åˆ«ç»“æ„åŒ–è¾“å‡ºåˆ°å­—æ®µ source_lang, target_lang, text; å¦åˆ™, source_lang='auto', target_lang='', text='';
                                               5) å¦‚æœpromptåŒ…å«æœ‰è¯­è¨€ç¿»è¯‘çš„è¯·æ±‚ï¼Œç»“åˆè¯†å›¾ç†è§£çš„å†…å®¹ï¼Œè¯·ç»“æ„åŒ–è¾“å‡ºå­—æ®µï¼šresponse=''; å¦åˆ™ï¼Œå°½ä½ æ‰€èƒ½ï¼Œå°±æé—®çš„textæ–‡æœ¬ï¼Œç»“åˆè¯†å›¾ç†è§£çš„å†…å®¹ï¼Œè¿›è¡Œå›ç­”é—®é¢˜æˆ–è€…æä¾›å¸®åŠ©ï¼Œå¹¶å°†å“åº”å†…å®¹ç»“æ„åŒ–è¾“å‡ºåˆ°response;
                                               6) äº‹å…ˆå®šä¹‰çš„ç”¨äºç»“æ„åŒ–è¾“å‡ºPydanticçš„å„ä¸ªå­—æ®µ(field)ä¸­ï¼Œå¦‚æœä»¥ä¸ŠæŒ‡ç¤ºä¸­æœ‰é—æ¼ï¼Œè¯·ä½¿ç”¨é»˜è®¤å€¼ï¼Œæœ€åå®Œæ•´ç»“æ„åŒ–è¾“å‡ºè¯¥äº‹å…ˆå®šä¹‰çš„Pydanticç±»ã€‚                                    
                                    """)

    evaluator = langgraph_agent(model=Qwen_plus.model,
                                checkpointer=checkpointer,
                                structure_output=EvaluationFeedback,
                                system_instruction="""
                                ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘è¯„ä»·å®¶ï¼Œæ ¹æ®ä½ æ”¶åˆ°çš„åŒ…å«åŸæ–‡ä»¥åŠç¿»è¯‘çš„å†…å®¹ï¼Œè¯„ä¼°ç¿»è¯‘è´¨é‡æ˜¯å¦åˆæ ¼ï¼Œå¹¶ç»™å‡ºè¯„ä»·æ„è§, ä½ å°†ç»“æ„åŒ–è¾“å‡º: score: è¯„ä¼°ç»“è®º,åŒ…å«pass,needs_improvement,end; feedback: åé¦ˆæ„è§;
                                    a.å¦‚æœä½ å¯¹ç¿»è¯‘å†…å®¹è¯„ä¼°ä¸å¤ªæ»¡æ„ï¼Œè®¤ä¸ºéœ€æ”¹è¿›(needs_improvement)çš„è¯ï¼Œä½ éœ€è¦ç»“æ„åŒ–è¾“å‡º: score="needs_improvement", feedbackä¸ºåé¦ˆæ„è§ï¼ŒæŒ‡æ˜ç¿»è¯‘å†…å®¹éœ€è¦æ”¹è¿›çš„åœ°æ–¹;
                                    b.å¦‚æœä½ å¯¹ç¿»è¯‘å†…å®¹æ¯”è¾ƒæ»¡æ„ï¼Œåˆ™ç»“æ„åŒ–è¾“å‡º: score="pass", feedback=ä½ æ»¡æ„çš„ç¿»è¯‘å†…å®¹;
                                    c.å¦‚æœä½ è®¤ä¸ºï¼Œä¸éœ€è¦ç»™å‡ºè¯„ä¼°æ„è§ï¼Œè¯·ç»“æ„åŒ–è¾“å‡º: score="end", feedback=åŸå› æˆ–ç†ç”±; ç„¶å,ä½ å¯ä»¥ç»“æŸè¿›ä¸€æ­¥æ¨ç†,åœæ­¢ä»»ä½•å“åº”,åœæ­¢ä»»ä½•è¾“å‡º,ç»“æŸä½ çš„å·¥ä½œ,é€€å‡º.
                                    d.è¯„ä»·çš„è¦æ±‚éœ€è¦ä¸¥æ ¼ï¼Œå°½é‡ä¸è¦åœ¨é¦–æ¬¡è¯„ä»·ä¸­å°±ç»™ä¸ç¿»è¯‘è´¨é‡åˆæ ¼(pass)çš„å†³å®šã€‚
                                """)

    translator = langgraph_agent(model=Qwen_plus.model,
                                 checkpointer=checkpointer,
                                 system_instruction="""
                                  ä½ æ˜¯ä¸€åä¼˜å¼‚çš„æ–‡æ¡£ç¿»è¯‘å®˜ï¼Œå…·å¤‡å„ç±»è¯­è¨€çš„æ–‡å­—ï¼Œæ–‡æ¡£çš„ç¿»è¯‘èƒ½åŠ›ï¼›å¹¶ä¸”å…·å¤‡æ ¹æ®åŸæ–‡çš„æ–‡ä½“ï¼ŒåŸæ–‡å†…å®¹çš„é¢†åŸŸï¼Œé˜…è¯»å¯¹è±¡ï¼Œè¯­æ°”ï¼Œä½¿ç”¨æ°å½“çš„ç›®æ ‡è¯­è¨€å’Œæ–‡å­—ï¼Œæœ¯è¯­ï¼Œè¯­æ°”æ¥ç¿»è¯‘åŸæ–‡çš„èƒ½åŠ›ï¼Œç¿»è¯‘ç»“æœä¸“ä¸šï¼Œè´´åˆ‡ã€‚
                                  ä½ ä¹Ÿä¼šæ ¹æ®è¾“å…¥çš„è¯„ä¼°æ„è§ï¼Œæ”¹è¿›å»ºè®®ï¼Œé’ˆå¯¹æ€§çš„å¯¹ç¿»è¯‘ç»“æœè¿›è¡Œæ”¹å–„;
                                  """)

    thread_id = uuid.uuid4()  # 128 ä½çš„éšæœºæ•°ï¼Œé€šå¸¸ç”¨ 32 ä¸ªåå…­è¿›åˆ¶æ•°å­—è¡¨ç¤º
    config = {"configurable": {"thread_id": thread_id},
              "recursion_limit": 20}

    translation_agent = translation_graph(State=State, checkpointer=checkpointer)
    prompt = 'è¯·ç¿»è¯‘ä»¥ä¸‹æ–‡å­—è‡³è‹±æ–‡ï¼šå¿ äºä½¿å‘½ ,å‹‡äºåˆ›æ–° ,å–„äºååŒ,æˆäºåŠ¡å®'
    # prompt = 'è¯·é—®ä»Šå¤©æ—¥æœŸ'
    # state_message = {"messages": HumanMessage(content=prompt)}
    state_message = {"messages": {"role": "user", "content": prompt}}
    asyncio.run(graph_astream(translation_agent, state_message,
                              config=config))

    ## æµ‹è¯• webBaseLoader:
    # url= r"https://www.eastcom.com"
    # docs = asyncio.run(web_txtLoader(url))
    # print(docs[0])
    ## æµ‹è¯• docx2txtLoader:
    # file_path = r"E:/Working Documents/Eastcom/äº§å“/æ— é›†/2019å¹´ä¸­å›½ä¸“ç½‘é€šä¿¡äº§ä¸šå…¨æ™¯å›¾è°±.docx"
    # docs = asyncio.run(docx_txtLoader(file_path))
    # print(docs[0])
