{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8accbf0e-4bdc-41e5-b80f-be75e72bee6a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-21T05:23:27.069210Z",
     "iopub.status.busy": "2023-12-21T05:23:27.068756Z",
     "iopub.status.idle": "2023-12-21T05:23:33.483149Z",
     "shell.execute_reply": "2023-12-21T05:23:33.482221Z",
     "shell.execute_reply.started": "2023-12-21T05:23:27.069181Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-21 13:23:29.810349: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-21 13:23:30.807550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, TrainingArguments, Trainer\n",
    "# from modelscope import snapshot_download\n",
    "# model_dir = snapshot_download(\"ZhipuAI/chatglm3-6b\", revision = \"v1.0.0\")\n",
    "# print(model_dir)\n",
    "model_dir = \"/mnt/workspace/.cache/modelscope/ZhipuAI/chatglm3-6b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56356e7e-2f97-4061-835e-72ab43942d7d",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2023-12-21T05:23:35.852579Z",
     "iopub.status.busy": "2023-12-21T05:23:35.851510Z",
     "iopub.status.idle": "2023-12-21T05:24:54.535325Z",
     "shell.execute_reply": "2023-12-21T05:24:54.534614Z",
     "shell.execute_reply.started": "2023-12-21T05:23:35.852545Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [01:09<00:00,  9.92s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "# model.half()与 load_in_8bit只能二选一;load_in_8bit=True时,已经将模型送入GPU.\n",
    "model = AutoModel.from_pretrained(model_dir, load_in_8bit=True, trust_remote_code=True, device_map ='auto')#.half().cuda()\n",
    "model = model.eval()\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0199e78-c9a0-4dc0-ac45-138ce4a84881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T05:25:10.897015Z",
     "iopub.status.busy": "2023-12-21T05:25:10.896512Z",
     "iopub.status.idle": "2023-12-21T05:25:11.508734Z",
     "shell.execute_reply": "2023-12-21T05:25:11.507828Z",
     "shell.execute_reply.started": "2023-12-21T05:25:10.896985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 21 13:25:11 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:08.0 Off |                    0 |\n",
      "| N/A   33C    P0    65W / 300W |   7922MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c2089d6f-5b3b-4872-98d9-67a1ccede199",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-21T07:20:52.107917Z",
     "iopub.status.busy": "2023-12-21T07:20:52.107434Z",
     "iopub.status.idle": "2023-12-21T07:20:52.113475Z",
     "shell.execute_reply": "2023-12-21T07:20:52.112785Z",
     "shell.execute_reply.started": "2023-12-21T07:20:52.107889Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference_fn(q, max_length, LoRA_flag=False):\n",
    "    \"\"\"\n",
    "    用于实现base model的推理\n",
    "    q: query\n",
    "    max_length:输出的长度\n",
    "    LoRA_flag: 是否是LoRA微调之后的模型. 当为True时,需要model.base_model.generate()\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(q, return_tensors='pt').to('cuda')\n",
    "    # print(f'base_model_input_ids:\\n{input_ids}')\n",
    "    model.eval()\n",
    "    if LoRA_flag:\n",
    "        output_ids = model.base_model.generate(input_ids['input_ids'],max_length=max_length)\n",
    "    else:\n",
    "        output_ids = model.generate(input_ids['input_ids'],max_length=max_length)\n",
    "    # print(f'base_model_output_ids:\\n{output_ids}')\n",
    "    response_ids = output_ids[0,input_ids['input_ids'].shape[-1]:]\n",
    "    response = tokenizer.decode(response_ids,skip_special_tokens=False)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "67a3eb48-2cb3-4564-ab12-4f734e8f80c0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-21T06:16:07.172028Z",
     "iopub.status.busy": "2023-12-21T06:16:07.171533Z",
     "iopub.status.idle": "2023-12-21T06:17:17.595057Z",
     "shell.execute_reply": "2023-12-21T06:17:17.594265Z",
     "shell.execute_reply.started": "2023-12-21T06:16:07.171998Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base mode response:\n",
      "\n",
      " 可以使用 Python 内置的 `len()` 函数来查看字典的长度。下面是一个简单的示例代码:\n",
      "\n",
      "```python\n",
      "my_dict = {'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}\n",
      "dict_length = len(my_dict)\n",
      "print('字典的长度为:', dict_length)\n",
      "```\n",
      "\n",
      "在这个示例中,我们首先创建了一个名为 `my_dict` 的字典,并使用 `len()` 函数计算其长度,将其存储在变量 `dict_length` 中。最后,我们使用 `print()` 函数输出字典的长度。\n",
      "\n",
      "输出结果应该是:\n",
      "\n",
      "```\n",
      "字典的长度为: 3\n",
      "```\n",
      "\n",
      "因为 `my_dict` 包含三个键值对。\n",
      "chat model response:\n",
      "在 Python 中，可以使用 `len()` 函数来查看字典的长度。下面是一个简单的例子：\n",
      "\n",
      "```python\n",
      "# 创建一个字典\n",
      "my_dict = {'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}\n",
      "\n",
      "# 获取字典的长度\n",
      "dict_length = len(my_dict)\n",
      "\n",
      "# 输出字典的长度\n",
      "print(\"字典的长度为：\", dict_length)\n",
      "```\n",
      "\n",
      "在这个例子中，我们首先创建了一个名为 `my_dict` 的字典，然后使用 `len()` 函数来获取其长度，并将结果存储在变量 `dict_length` 中。最后，我们使用 `print()` 函数将字典的长度输出到控制台。\n"
     ]
    }
   ],
   "source": [
    "q = \"python-查看字典的长度,如何?\"\n",
    "\n",
    "# base_model模式: 不需要role\n",
    "response = inference_fn(q,max_length=8192)\n",
    "print(f\"base mode response:\\n{response}\")\n",
    "# chat_model模式: 可以多role\n",
    "response, history = model.chat(tokenizer, q, history=history)\n",
    "response, history = model.chat(tokenizer, query=q, history= history, role= \"user\",  max_length = 8192, num_beams=1, do_sample=True, top_p=0.8, temperature=0.8,)\n",
    "print(f\"chat model response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca77ba79-942f-4c8b-bd85-091f4188baac",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-21T05:28:21.248612Z",
     "iopub.status.busy": "2023-12-21T05:28:21.248146Z",
     "iopub.status.idle": "2023-12-21T05:28:21.388218Z",
     "shell.execute_reply": "2023-12-21T05:28:21.387525Z",
     "shell.execute_reply.started": "2023-12-21T05:28:21.248585Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<unk>\n",
      "<unk>\n",
      "eos的token ids: 2\n",
      "ids: {'input_ids': [64790, 64792, 19079, 30941, 33367, 52679, 35465, 54648, 30932, 31750, 30987], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'position_ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
      "ids_encode: [64790, 64792, 19079, 30941, 33367, 52679, 35465, 54648, 30932, 31750, 30987]\n",
      "text_decode: [gMASK]sop python-查看字典的长度,如何?\n",
      "vocal_size: 64798\n",
      "token: 0\n",
      "tokens_to_string: 美丽的花朵\n",
      "<bound method ChatGLMTokenizer.get_prefix_tokens of ChatGLMTokenizer(name_or_path='/mnt/workspace/.cache/modelscope/ZhipuAI/chatglm3-6b', vocab_size=64798, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t\n",
      "}>\n",
      "build_single_message:[64796, 1429, 13, 19079, 30941, 33367, 52679, 35465, 54648, 30932, 31750, 30987]\n",
      "<|assistant|> test\n",
      " python-查看字典的长度,如何?\n",
      "{'input_ids': tensor([[64790, 64792, 64795, 30910,    13, 19079, 30941, 33367, 52679, 35465,\n",
      "         54648, 30932, 31750, 30987, 64796]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]])}\n",
      "[gMASK]sop<|user|> \n",
      " python-查看字典的长度,如何?<|assistant|>\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer(q)\n",
    "ids_encode = tokenizer.encode(q, add_special_tokens=True)\n",
    "ids_decode = tokenizer.decode(ids_encode, skip_special_tokens=False)\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "print(tokenizer.get_command(\"<pad>\"))\n",
    "print(tokenizer.unk_token)\n",
    "print(tokenizer.pad_token)\n",
    "print(f\"eos的token ids: {tokenizer.eos_token_id}\")\n",
    "print(f\"ids: {ids}\")\n",
    "print(f\"ids_encode: {ids_encode}\")\n",
    "print(f\"text_decode: {ids_decode}\")\n",
    "print(f\"vocal_size: {vocab_size}\")\n",
    "token_id = tokenizer._convert_token_to_id('四书') \n",
    "print(f\"token: {token_id}\")\n",
    "tokens_string= tokenizer.convert_tokens_to_string(['美丽','的','花朵'])\n",
    "print(f\"tokens_to_string: {tokens_string}\")\n",
    "print(tokenizer.get_prefix_tokens)\n",
    "single_message = tokenizer.build_single_message( role='assistant', metadata='test', message=q)\n",
    "print(f'build_single_message:{single_message}')\n",
    "print(tokenizer.decode(single_message))\n",
    "chat_input = tokenizer.build_chat_input(query=q, history=None, role=\"user\")\n",
    "print(chat_input)\n",
    "print(tokenizer.decode(chat_input['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cebedc-b25d-496e-b25b-85509d4a97c7",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a09a4aa-1033-40f1-9359-8e01e28264d5",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-21T06:17:58.929921Z",
     "iopub.status.busy": "2023-12-21T06:17:58.929317Z",
     "iopub.status.idle": "2023-12-21T06:17:58.935806Z",
     "shell.execute_reply": "2023-12-21T06:17:58.935055Z",
     "shell.execute_reply.started": "2023-12-21T06:17:58.929893Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "datas = [\n",
    "{\n",
    "    \"context\": \"大师，我最近事业不顺利，感觉很迷茫，该怎么办？\",\n",
    "    \"target\": \"孩子，生活中的起伏和挫折是常态。放下烦恼，用平静的心态面对困境，努力奋斗，同时保持内心的善良与慈悲，你将找到属于你的光明之路。\",\n",
    "},\n",
    "{\n",
    "    \"context\": \"师父，我与家人之间常常发生争吵，如何改善家庭关系？\",\n",
    "    \"target\": \"阿弥陀佛，亲情是一份宝贵的财富，要从宽容和理解开始。培养平和的心态，倾听彼此的意见，以慈悲与智慧去化解冲突，共同创造和谐温暖的家庭氛围。\",\n",
    "},\n",
    "{\n",
    "    \"context\": \"师傅，我经常感到焦虑和压力，怎样才能获得内心的宁静？\",\n",
    "    \"target\": \"孩子，内心的宁静源于觉察和自我调适。修行冥想，深呼吸，寻找内心的栖息地。明白外在的变幻无常，放下过多的执着与欲望，你将发现内心的平静与安宁。\",\n",
    "},\n",
    "{\n",
    "    \"context\": \"大师，我常常陷入烦恼和抱怨，怎样才能培养感恩之心？\",\n",
    "    \"target\": \"施主，感恩是一种修养，需要从日常小事开始。每天意识到自己的幸运和获得，体察他人的善意和关爱。珍惜当下，感恩生活中的点滴，你将发现更多的快乐与满足。\",\n",
    "},\n",
    "{\n",
    "    \"context\": \"师父，我对未来感到迷茫和恐惧，不知道自己的人生目标是什么，有何建议？\",\n",
    "    \"target\": \"阿弥陀佛，人生的方向不一定要一目了然，它是通过不断尝试和探索而逐渐清晰的。保持对未知的勇敢与开放，寻找自己内心的声音。用心去体验，用智慧去选择，你将找到属于自己的道路。\",\n",
    "}]\n",
    "\n",
    "max_prompt_len = max([len(data['context']) for data in datas]) + 5\n",
    "max_response_len = max([len(data['target']) for data in datas]) + 5\n",
    "max_seq_length = max_prompt_len + max_response_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc75ed-c69f-46cf-8182-20a8b97ad89c",
   "metadata": {},
   "source": [
    "[<font color=lime size=6 face=lisu>github: ChatGLM3-6B 微调示例</font>](https://github.com/THUDM/ChatGLM3/tree/main/finetune_chatmodel_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9168a466-4319-4463-b0f0-ab40f46c41c7",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-21T06:41:05.388000Z",
     "iopub.status.busy": "2023-12-21T06:41:05.387510Z",
     "iopub.status.idle": "2023-12-21T06:41:05.396961Z",
     "shell.execute_reply": "2023-12-21T06:41:05.396218Z",
     "shell.execute_reply.started": "2023-12-21T06:41:05.387970Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from transformers import PreTrainedTokenizer, DataCollatorForSeq2Seq\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InputOutputDataset(Dataset):\n",
    "    def __init__(self, data: List[dict], tokenizer: PreTrainedTokenizer, max_source_length: int, max_target_length: int):\n",
    "        super(InputOutputDataset, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        self.max_seq_length = max_source_length + max_target_length + 1\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i) -> dict:\n",
    "        data_item = self.data[i]\n",
    "\n",
    "        a_ids = self.tokenizer.encode(text=data_item['context'], add_special_tokens=True, truncation=True,\n",
    "                                         max_length=self.max_source_length)\n",
    "        b_ids = self.tokenizer.encode(text=data_item['target'], add_special_tokens=False, truncation=True,\n",
    "                                    max_length=self.max_target_length)\n",
    "\n",
    "        context_length = len(a_ids)\n",
    "        input_ids = a_ids + b_ids + [self.tokenizer.eos_token_id]\n",
    "        labels = [self.tokenizer.pad_token_id] * context_length + b_ids + [self.tokenizer.eos_token_id]\n",
    "        \n",
    "        pad_len = self.max_seq_length - len(input_ids)\n",
    "        input_ids = input_ids + [self.tokenizer.pad_token_id] * pad_len\n",
    "        labels = labels + [self.tokenizer.pad_token_id] * pad_len\n",
    "        labels = [(l if l != self.tokenizer.pad_token_id else -100) for l in labels]\n",
    "\n",
    "        assert len(input_ids) == len(labels), f\"length mismatch: {len(input_ids)} vs {len(labels)}\"\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e6337b9-f780-49c7-a608-f6cb2f64a937",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-21T06:41:10.981365Z",
     "iopub.status.busy": "2023-12-21T06:41:10.980862Z",
     "iopub.status.idle": "2023-12-21T06:41:10.988247Z",
     "shell.execute_reply": "2023-12-21T06:41:10.987537Z",
     "shell.execute_reply.started": "2023-12-21T06:41:10.981334Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [64790, 64792, 33697, 54758, 31123, 54546, 32483, 32018, 54535, 32790, 31123, 32044, 54657, 40739, 31123, 49086, 31514, 30910, 31652, 31123, 37818, 38907, 54542, 38625, 54532, 34611, 31155, 35685, 35644, 31123, 54571, 37129, 38888, 32122, 36198, 31123, 31862, 32721, 31123, 31701, 31983, 37005, 35178, 54619, 42049, 31123, 54622, 54687, 32523, 32180, 31822, 34702, 33532, 31155, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 31652, 31123, 37818, 38907, 54542, 38625, 54532, 34611, 31155, 35685, 35644, 31123, 54571, 37129, 38888, 32122, 36198, 31123, 31862, 32721, 31123, 31701, 31983, 37005, 35178, 54619, 42049, 31123, 54622, 54687, 32523, 32180, 31822, 34702, 33532, 31155, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n",
      "[gMASK]sop 大师，我最近事业不顺利，感觉很迷茫，该怎么办？ 孩子，生活中的起伏和挫折是常态。放下烦恼，用平静的心态面对困境，努力奋斗，同时保持内心的善良与慈悲，你将找到属于你的光明之路。\n"
     ]
    }
   ],
   "source": [
    "dataset = InputOutputDataset(data=datas, tokenizer=tokenizer, max_source_length=max_prompt_len, max_target_length=max_response_len)\n",
    "# dataset = MultiTurnDataset(data=datas, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
    "print(dataset[0])\n",
    "print(tokenizer.decode(dataset[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "219976e1-aa93-4e49-a079-3ac722bce789",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2023-12-21T06:43:22.549833Z",
     "iopub.status.busy": "2023-12-21T06:43:22.549345Z",
     "iopub.status.idle": "2023-12-21T06:43:27.361238Z",
     "shell.execute_reply": "2023-12-21T06:43:27.360489Z",
     "shell.execute_reply.started": "2023-12-21T06:43:22.549804Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model,prepare_model_for_kbit_training\n",
    "\n",
    "# 确保梯度检查点和模型并行化设置正确\n",
    "#model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.is_parallelizable = True\n",
    "model.model_parallel = True  # 可以尝试暂时关闭模型并行化来看是否解决问题\n",
    "# model.lm_head = CastOutputToFloat(model.transformer.output_layer)\n",
    "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
    "\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['query_key_value'],\n",
    "    inference_mode=False,\n",
    "       )\n",
    "\n",
    "model = get_peft_model(model, peft_config).to('cuda')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"output\",\n",
    "    # fp16 =True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size = 2,\n",
    "    learning_rate = 1e-4,\n",
    "    num_train_epochs=100,\n",
    "    logging_steps=10,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=['labels'],\n",
    "    seed=0,\n",
    "    data_seed=0,\n",
    "    group_by_length=False,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100,\n",
    "        pad_to_multiple_of=None,\n",
    "        padding=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f3c65ea5-8dde-44c6-80b8-fe63343c4ffb",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-21T06:43:31.211113Z",
     "iopub.status.busy": "2023-12-21T06:43:31.210615Z",
     "iopub.status.idle": "2023-12-21T06:51:35.577950Z",
     "shell.execute_reply": "2023-12-21T06:51:35.577257Z",
     "shell.execute_reply.started": "2023-12-21T06:43:31.211083Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 08:02, Epoch 150/150]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.708600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.759700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.827000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.357100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.064400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=450, training_loss=0.2176804869999695, metrics={'train_runtime': 484.0246, 'train_samples_per_second': 1.55, 'train_steps_per_second': 0.93, 'total_flos': 3470922925056000.0, 'train_loss': 0.2176804869999695, 'epoch': 150.0})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "79e9bf9a-5aaf-476a-99bc-cd7c7f77147a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-21T08:14:26.918740Z",
     "iopub.status.busy": "2023-12-21T08:14:26.918223Z",
     "iopub.status.idle": "2023-12-21T08:15:21.274160Z",
     "shell.execute_reply": "2023-12-21T08:15:21.273437Z",
     "shell.execute_reply.started": "2023-12-21T08:14:26.918710Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base mode response:\n",
      "孩子，我会告诉你一些与同事相处的技巧。首先，要诚实、友好和尊重他人。其次，要善于倾听他人的意见，并学会表达自己的观点。最后，要勇于承担责任，并尝试与他人建立良好的团队关系。\n",
      "chat model response:\n",
      "以下是一些建议:\n",
      "\n",
      "1. 积极沟通:沟通是建立良好关系的关键。尝试与同事进行积极的对话,分享自己的想法和观点,并听取他们的看法。如果存在分歧,尝试以合作的方式解决问题。\n",
      "\n",
      "2. 尊重他人:尊重是建立良好关系的基础。尊重同事的意见和观点,不要对他们的想法进行批评或攻击。同时,也要尊重同事的隐私和个人空间。\n",
      "\n",
      "3. 诚实坦率:诚实是建立信任和良好关系的关键。尝试与同事坦诚相待,分享自己的想法和感受,同时也要接受他们的反馈和建议。\n",
      "\n",
      "4. 建立共同目标:与同事建立共同的目标和价值观可以加强团队凝聚力和合作精神。尝试与同事一起制定目标并努力实现它们。\n",
      "\n",
      "5. 学会妥协:在人际关系中,妥协是必要的。尝试与同事一起找到一个共同的解决方案,以满足双方的需求和利益。\n",
      "\n",
      "6. 培养团队合作精神:团队合作精神是建立良好关系的关键。尝试与同事一起工作,分享资源和知识,以实现更好的结果。\n",
      "\n",
      "7. 尊重不同的文化和背景:如果同事来自不同的文化或背景,尝试理解他们的文化和价值观。尊重他们的差异,并尝试与他们在相互尊重的基础上建立关系。\n",
      "\n",
      "改善与同事的关系需要积极的努力和沟通。尝试遵循这些建议,以建立更好的工作关系。\n"
     ]
    }
   ],
   "source": [
    "model.config.use_cache = True\n",
    "history=[]\n",
    "q = \"大师，我会为与同事的关系相处而苦恼,如何改善与同事的关系呢?\"\n",
    "\n",
    "# base_model模式: 不需要role\n",
    "response = inference_fn(q,max_length=1024, LoRA_flag=True)\n",
    "print(f\"base mode response:\\n{response}\")\n",
    "# chat_model模式: 可以多role\n",
    "response, history = model.chat(tokenizer, q, history=history)\n",
    "response, history = model.chat(tokenizer, query=q, history= history, role= \"user\",  max_length = 8192, num_beams=1, do_sample=True, top_p=0.8, temperature=0.8,)\n",
    "print(f\"chat model response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ab3ab-9e65-402d-99ba-6753095e7673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
